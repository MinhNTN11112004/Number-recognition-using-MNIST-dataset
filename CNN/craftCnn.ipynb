{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "RYLAuhYvTuxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jddi1jhRTp3H"
      },
      "outputs": [],
      "source": [
        "class Convolution:\n",
        "    def __init__(self, num_filters, filter_size, num_channels, learning_rate=0.01, momentum=0.9):\n",
        "        self.num_filters = num_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.num_channels = num_channels\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.filters = np.random.randn(num_filters, num_channels, filter_size, filter_size) * np.sqrt(2.0 / (num_channels * filter_size * filter_size))\n",
        "        self.biases = np.zeros((num_filters, 1))\n",
        "        self.velocity_filters = np.zeros_like(self.filters)\n",
        "        self.velocity_biases = np.zeros_like(self.biases)\n",
        "\n",
        "    def iterate_regions(self, image):\n",
        "        Cin, h, w = image.shape\n",
        "        for i in range(h - self.filter_size + 1):\n",
        "            for j in range(w - self.filter_size + 1):\n",
        "                region = image[:, i:(i + self.filter_size), j:(j + self.filter_size)]\n",
        "                yield region, i, j\n",
        "\n",
        "    def forward(self, input_images):\n",
        "        self.last_input = input_images\n",
        "        batch_size, Cin, h, w = input_images.shape\n",
        "        output_height = h - self.filter_size + 1\n",
        "        output_width = w - self.filter_size + 1\n",
        "        output = np.zeros((batch_size, self.num_filters, output_height, output_width))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for region, i, j in self.iterate_regions(input_images[b]):\n",
        "                for f in range(self.num_filters):\n",
        "                    output[b, f, i, j] = np.sum(region * self.filters[f]) + self.biases[f]\n",
        "\n",
        "        # ReLU activation\n",
        "        self.last_output = np.maximum(0, output)\n",
        "        return self.last_output\n",
        "\n",
        "    def backward(self, d_L_d_out):\n",
        "        d_L_d_out[self.last_output <= 0] = 0  # ReLU backpropagation\n",
        "\n",
        "        batch_size = d_L_d_out.shape[0]\n",
        "        d_L_d_filters = np.zeros(self.filters.shape)\n",
        "        d_L_d_biases = np.zeros(self.biases.shape)\n",
        "        d_L_d_input = np.zeros(self.last_input.shape)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for region, i, j in self.iterate_regions(self.last_input[b]):\n",
        "                for f in range(self.num_filters):\n",
        "                    d_L_d_filters[f] += d_L_d_out[b, f, i, j] * region\n",
        "                    d_L_d_biases[f] += d_L_d_out[b, f, i, j]\n",
        "                    d_L_d_input[b, :, i:(i + self.filter_size), j:(j + self.filter_size)] += d_L_d_out[b, f, i, j] * self.filters[f]\n",
        "\n",
        "        # Update with momentum\n",
        "        self.velocity_filters = self.momentum * self.velocity_filters - self.learning_rate * d_L_d_filters\n",
        "        self.velocity_biases = self.momentum * self.velocity_biases - self.learning_rate * d_L_d_biases\n",
        "        self.filters += self.velocity_filters\n",
        "        self.biases += self.velocity_biases\n",
        "\n",
        "        return d_L_d_input\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Pooling:\n",
        "    def __init__(self, pool_size=2, stride=2):\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def iterate_regions(self, image):\n",
        "        h, w = image.shape[-2:]\n",
        "        for i in range(0, h - self.pool_size + 1, self.stride):\n",
        "            for j in range(0, w - self.pool_size + 1, self.stride):\n",
        "                region = image[i:(i + self.pool_size), j:(j + self.pool_size)]\n",
        "                yield region, i // self.stride, j // self.stride\n",
        "\n",
        "    def forward(self, input_images):\n",
        "        self.last_input = input_images\n",
        "        batch_size, num_filters, h, w = input_images.shape\n",
        "        output_height = (h - self.pool_size) // self.stride + 1\n",
        "        output_width = (w - self.pool_size) // self.stride + 1\n",
        "        output = np.zeros((batch_size, num_filters, output_height, output_width))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for f in range(num_filters):\n",
        "                for region, i, j in self.iterate_regions(input_images[b, f]):\n",
        "                    output[b, f, i, j] = np.max(region)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, d_L_d_out):\n",
        "        d_L_d_input = np.zeros(self.last_input.shape)\n",
        "        batch_size, num_filters, output_height, output_width = d_L_d_out.shape\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for f in range(num_filters):\n",
        "                for region, i, j in self.iterate_regions(self.last_input[b, f]):\n",
        "                    max_val = np.max(region)\n",
        "                    m, n = np.unravel_index(np.argmax(region), region.shape)\n",
        "                    d_L_d_input[b, f, i * self.stride + m, j * self.stride + n] = d_L_d_out[b, f, i, j]\n",
        "\n",
        "        return d_L_d_input\n"
      ],
      "metadata": {
        "id": "MXska8JgVNOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten:\n",
        "    def forward(self, input_tensor):\n",
        "\n",
        "        self.input_shape = input_tensor.shape\n",
        "\n",
        "        batch_size = input_tensor.shape[0]\n",
        "        return input_tensor.reshape(batch_size, -1)\n",
        "\n",
        "    def backward(self, d_L_d_out):\n",
        "\n",
        "        return d_L_d_out.reshape(self.input_shape)"
      ],
      "metadata": {
        "id": "uM8ERgM3VSgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense:\n",
        "    def __init__(self, input_size, output_size, learning_rate=0.01, momentum=0.9):\n",
        "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)\n",
        "        self.biases = np.zeros(output_size)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity_weights = np.zeros_like(self.weights)\n",
        "        self.velocity_biases = np.zeros_like(self.biases)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        self.last_input = input_tensor\n",
        "        return np.dot(input_tensor, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, d_L_d_out):\n",
        "        d_L_d_weights = np.dot(self.last_input.T, d_L_d_out)\n",
        "        d_L_d_biases = np.sum(d_L_d_out, axis=0)\n",
        "        d_L_d_input = np.dot(d_L_d_out, self.weights.T)\n",
        "\n",
        "        self.velocity_weights = self.momentum * self.velocity_weights - self.learning_rate * d_L_d_weights\n",
        "        self.velocity_biases = self.momentum * self.velocity_biases - self.learning_rate * d_L_d_biases\n",
        "        self.weights += self.velocity_weights\n",
        "        self.biases += self.velocity_biases\n",
        "\n",
        "        return d_L_d_input"
      ],
      "metadata": {
        "id": "TgI14JOXVgqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def __init__(self, dropout_rate=0.2):\n",
        "        self.input_tensor = None\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, input_tensor, training=True):\n",
        "        self.input_tensor = input_tensor\n",
        "        if training:\n",
        "\n",
        "            self.mask = np.random.binomial(1, 1 - self.dropout_rate, size=input_tensor.shape)\n",
        "            output = np.maximum(0, input_tensor) * self.mask\n",
        "\n",
        "            output = output / (1 - self.dropout_rate)\n",
        "        else:\n",
        "            output = np.maximum(0, input_tensor) * (1 - self.dropout_rate)\n",
        "        return output\n",
        "\n",
        "    def backward(self, d_L_d_out):\n",
        "        d_L_d_input = d_L_d_out * (self.input_tensor > 0) * self.mask\n",
        "        d_L_d_input /= (1 - self.dropout_rate)\n",
        "        return d_L_d_input\n"
      ],
      "metadata": {
        "id": "X3BzNTJxW-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.probs = None\n",
        "        self.labels = None\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        self.labels = labels\n",
        "\n",
        "        exp_vals = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        self.probs = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
        "\n",
        "        batch_size = logits.shape[0]\n",
        "\n",
        "        correct_log_probs = -np.log(self.probs[range(batch_size), self.labels])\n",
        "        loss = np.sum(correct_log_probs) / batch_size\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        batch_size = self.labels.shape[0]\n",
        "\n",
        "        d_L_d_logits = self.probs\n",
        "        d_L_d_logits[range(batch_size), self.labels] -= 1\n",
        "        d_L_d_logits /= batch_size\n",
        "\n",
        "        return d_L_d_logits\n"
      ],
      "metadata": {
        "id": "fcoBrsZ0nIny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN:\n",
        "    def __init__(self):\n",
        "        # 32x1x28x28\n",
        "        self.conv1 = Convolution(num_filters=8, filter_size=3, num_channels=1)\n",
        "        # 32x8x26x26\n",
        "        self.pool1 = Pooling(pool_size=2, stride=2)\n",
        "        # 32x8x13x13\n",
        "        self.conv2 = Convolution(num_filters=16, filter_size=3, num_channels=8)\n",
        "        # 32x16x11x11\n",
        "        self.pool2 = Pooling(pool_size=2, stride=2)\n",
        "        # 32x16x5x5\n",
        "        self.flatten = Flatten()\n",
        "        # 32x16*5*5=32x400\n",
        "        # Conv1 -> Pool1 -> Conv2 -> Pool2 results in 16 filters of 5x5 each.\n",
        "        # Thus, flattened output size is 16 * 5 * 5 = 400\n",
        "\n",
        "        self.fc1 = Dense(input_size=16 * 5 * 5, output_size=64)\n",
        "        # 32x64\n",
        "        self.relu = ReLU(dropout_rate=0.2)\n",
        "        self.fc2 = Dense(input_size=64, output_size=10)  # For 10 classes in MNIST\n",
        "        # 32x10\n",
        "        self.softmax_crossentropy = Softmax()\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x = self.conv1.forward(x)\n",
        "        x = self.pool1.forward(x)\n",
        "\n",
        "        x = self.conv2.forward(x)\n",
        "        x = self.pool2.forward(x)\n",
        "\n",
        "        x = self.flatten.forward(x)\n",
        "\n",
        "        x = self.fc1.forward(x)\n",
        "        x = self.relu.forward(x)\n",
        "\n",
        "        x = self.fc2.forward(x)\n",
        "        # đơn giản là các giá trị trước kích hoạt nếu là train thì cần tính loss không train dùng luôn không cần kích hoạt\n",
        "        if labels is not None:\n",
        "            loss = self.softmax_crossentropy.forward(x, labels)\n",
        "            return x, loss\n",
        "        return x\n",
        "\n",
        "    def backward(self):\n",
        "        d_L_d_logits = self.softmax_crossentropy.backward()\n",
        "\n",
        "        d_L_d_out = self.fc2.backward(d_L_d_logits)\n",
        "        d_L_d_out = self.relu.backward(d_L_d_out)\n",
        "        d_L_d_out = self.fc1.backward(d_L_d_out)\n",
        "\n",
        "        d_L_d_out = self.flatten.backward(d_L_d_out)\n",
        "\n",
        "        d_L_d_out = self.pool2.backward(d_L_d_out)\n",
        "        d_L_d_out = self.conv2.backward(d_L_d_out)\n",
        "\n",
        "        d_L_d_out = self.pool1.backward(d_L_d_out)\n",
        "        self.conv1.backward(d_L_d_out)\n",
        "\n",
        "    def train(self, train_images, train_labels, epochs=10, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "            batch_loss = 0\n",
        "\n",
        "            indices = np.random.permutation(len(train_images))\n",
        "            train_images = train_images[indices]\n",
        "            train_labels = train_labels[indices]\n",
        "\n",
        "            for i in range(0, len(train_images), batch_size):\n",
        "                batch_images = train_images[i:i+batch_size]\n",
        "                batch_labels = train_labels[i:i+batch_size]\n",
        "\n",
        "                _, loss = self.forward(batch_images, batch_labels)\n",
        "                batch_loss += loss\n",
        "\n",
        "                print(f\"Batch {i // batch_size + 1} Loss: {loss:.4f}\")\n",
        "\n",
        "                self.backward()\n",
        "\n",
        "            avg_loss = batch_loss / (len(train_images) // batch_size)\n",
        "            print(f\"Average Loss after Epoch {epoch + 1}: {avg_loss:.4f}\")\n",
        "\n",
        "    def predict(self, test_images):\n",
        "        logits = self.forward(test_images, None)\n",
        "        return np.argmax(logits, axis=1)\n"
      ],
      "metadata": {
        "id": "ZLicDXEfOaHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "test_images = test_images.astype(np.float32) / 255.0\n",
        "train_images = train_images.reshape(-1, 1, 28, 28)\n",
        "test_images = test_images.reshape(-1, 1, 28, 28)\n",
        "\n",
        "cnn = SimpleCNN()\n",
        "\n",
        "cnn.train(train_images, train_labels, epochs=1, batch_size=32)\n",
        "\n",
        "predictions = cnn.predict(test_images)\n",
        "accuracy = np.mean(predictions == test_labels)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mru-18CsOca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c9c453f-1b75-406a-9d49-6453d569affe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3fb70eaab3f5>:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  output[b, f, i, j] = np.sum(region * self.filters[f]) + self.biases[f]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 Loss: 2.9496\n",
            "Batch 2 Loss: 2.3364\n",
            "Batch 3 Loss: 2.4098\n",
            "Batch 4 Loss: 2.5159\n",
            "Batch 5 Loss: 2.2995\n",
            "Batch 6 Loss: 1.8165\n",
            "Batch 7 Loss: 2.2511\n",
            "Batch 8 Loss: 2.1517\n",
            "Batch 9 Loss: 2.1817\n",
            "Batch 10 Loss: 1.8040\n",
            "Batch 11 Loss: 1.9313\n",
            "Batch 12 Loss: 1.4931\n",
            "Batch 13 Loss: 1.8883\n",
            "Batch 14 Loss: 1.8007\n",
            "Batch 15 Loss: 1.6002\n",
            "Batch 16 Loss: 1.3678\n",
            "Batch 17 Loss: 1.3718\n",
            "Batch 18 Loss: 1.8548\n",
            "Batch 19 Loss: 1.8690\n",
            "Batch 20 Loss: 1.5685\n",
            "Batch 21 Loss: 1.7221\n",
            "Batch 22 Loss: 1.5259\n",
            "Batch 23 Loss: 1.1602\n",
            "Batch 24 Loss: 1.5360\n",
            "Batch 25 Loss: 1.2835\n",
            "Batch 26 Loss: 1.1604\n",
            "Batch 27 Loss: 1.2716\n",
            "Batch 28 Loss: 1.2311\n",
            "Batch 29 Loss: 0.8734\n",
            "Batch 30 Loss: 1.1046\n",
            "Batch 31 Loss: 1.0366\n",
            "Batch 32 Loss: 0.8770\n",
            "Batch 33 Loss: 0.8987\n",
            "Batch 34 Loss: 0.8078\n",
            "Batch 35 Loss: 0.9635\n",
            "Batch 36 Loss: 1.2535\n",
            "Batch 37 Loss: 0.6465\n",
            "Batch 38 Loss: 0.8014\n",
            "Batch 39 Loss: 0.8328\n",
            "Batch 40 Loss: 0.8850\n",
            "Batch 41 Loss: 1.1783\n",
            "Batch 42 Loss: 1.2090\n",
            "Batch 43 Loss: 0.9454\n",
            "Batch 44 Loss: 0.9768\n",
            "Batch 45 Loss: 0.9506\n",
            "Batch 46 Loss: 0.5367\n",
            "Batch 47 Loss: 0.7024\n",
            "Batch 48 Loss: 0.8146\n",
            "Batch 49 Loss: 1.0049\n",
            "Batch 50 Loss: 0.7114\n",
            "Batch 51 Loss: 0.9095\n",
            "Batch 52 Loss: 1.0269\n",
            "Batch 53 Loss: 1.1482\n",
            "Batch 54 Loss: 0.6661\n",
            "Batch 55 Loss: 0.7372\n",
            "Batch 56 Loss: 0.7064\n",
            "Batch 57 Loss: 0.3879\n",
            "Batch 58 Loss: 0.5641\n",
            "Batch 59 Loss: 0.5268\n",
            "Batch 60 Loss: 0.6685\n",
            "Batch 61 Loss: 0.9422\n",
            "Batch 62 Loss: 0.9131\n",
            "Batch 63 Loss: 0.8705\n",
            "Batch 64 Loss: 0.7262\n",
            "Batch 65 Loss: 0.5805\n",
            "Batch 66 Loss: 0.5322\n",
            "Batch 67 Loss: 0.5360\n",
            "Batch 68 Loss: 0.7242\n",
            "Batch 69 Loss: 0.5086\n",
            "Batch 70 Loss: 0.8020\n",
            "Batch 71 Loss: 0.6450\n",
            "Batch 72 Loss: 0.7797\n",
            "Batch 73 Loss: 0.7172\n",
            "Batch 74 Loss: 0.5589\n",
            "Batch 75 Loss: 0.6789\n",
            "Batch 76 Loss: 0.4823\n",
            "Batch 77 Loss: 0.4775\n",
            "Batch 78 Loss: 0.6782\n",
            "Batch 79 Loss: 0.3113\n",
            "Batch 80 Loss: 0.6553\n",
            "Batch 81 Loss: 0.6906\n",
            "Batch 82 Loss: 0.2524\n",
            "Batch 83 Loss: 0.3473\n",
            "Batch 84 Loss: 0.4930\n",
            "Batch 85 Loss: 0.3502\n",
            "Batch 86 Loss: 0.4768\n",
            "Batch 87 Loss: 0.4933\n",
            "Batch 88 Loss: 0.3605\n",
            "Batch 89 Loss: 0.5930\n",
            "Batch 90 Loss: 0.3788\n",
            "Batch 91 Loss: 0.4779\n",
            "Batch 92 Loss: 0.7484\n",
            "Batch 93 Loss: 0.4115\n",
            "Batch 94 Loss: 0.5508\n",
            "Batch 95 Loss: 0.5412\n",
            "Batch 96 Loss: 0.4504\n",
            "Batch 97 Loss: 0.4667\n",
            "Batch 98 Loss: 0.4262\n",
            "Batch 99 Loss: 0.5289\n",
            "Batch 100 Loss: 0.4170\n",
            "Batch 101 Loss: 0.5058\n",
            "Batch 102 Loss: 0.1773\n",
            "Batch 103 Loss: 0.3345\n",
            "Batch 104 Loss: 0.4360\n",
            "Batch 105 Loss: 0.2620\n",
            "Batch 106 Loss: 0.3145\n",
            "Batch 107 Loss: 0.1692\n",
            "Batch 108 Loss: 0.7014\n",
            "Batch 109 Loss: 0.4064\n",
            "Batch 110 Loss: 0.6221\n",
            "Batch 111 Loss: 0.1730\n",
            "Batch 112 Loss: 0.4576\n",
            "Batch 113 Loss: 0.8627\n",
            "Batch 114 Loss: 0.6913\n",
            "Batch 115 Loss: 0.4360\n",
            "Batch 116 Loss: 0.5094\n",
            "Batch 117 Loss: 0.5664\n",
            "Batch 118 Loss: 0.4528\n",
            "Batch 119 Loss: 0.6870\n",
            "Batch 120 Loss: 0.2494\n",
            "Batch 121 Loss: 0.3236\n",
            "Batch 122 Loss: 0.2982\n",
            "Batch 123 Loss: 0.6446\n",
            "Batch 124 Loss: 0.3336\n",
            "Batch 125 Loss: 0.4550\n",
            "Batch 126 Loss: 0.2115\n",
            "Batch 127 Loss: 0.4014\n",
            "Batch 128 Loss: 0.2913\n",
            "Batch 129 Loss: 0.4542\n",
            "Batch 130 Loss: 0.1473\n",
            "Batch 131 Loss: 0.2989\n",
            "Batch 132 Loss: 0.3633\n",
            "Batch 133 Loss: 0.5799\n",
            "Batch 134 Loss: 0.4300\n",
            "Batch 135 Loss: 0.3557\n",
            "Batch 136 Loss: 0.5724\n",
            "Batch 137 Loss: 0.4819\n",
            "Batch 138 Loss: 0.3074\n",
            "Batch 139 Loss: 0.3625\n",
            "Batch 140 Loss: 0.4860\n",
            "Batch 141 Loss: 0.9850\n",
            "Batch 142 Loss: 0.4342\n",
            "Batch 143 Loss: 0.2225\n",
            "Batch 144 Loss: 0.2111\n",
            "Batch 145 Loss: 0.3385\n",
            "Batch 146 Loss: 0.7602\n",
            "Batch 147 Loss: 0.3947\n",
            "Batch 148 Loss: 0.2616\n",
            "Batch 149 Loss: 0.5026\n",
            "Batch 150 Loss: 0.2111\n",
            "Batch 151 Loss: 0.3220\n",
            "Batch 152 Loss: 0.4276\n",
            "Batch 153 Loss: 0.6107\n",
            "Batch 154 Loss: 0.6495\n",
            "Batch 155 Loss: 0.5288\n",
            "Batch 156 Loss: 0.2791\n",
            "Batch 157 Loss: 0.3019\n",
            "Batch 158 Loss: 0.3376\n",
            "Batch 159 Loss: 0.3605\n",
            "Batch 160 Loss: 0.2250\n",
            "Batch 161 Loss: 0.2418\n",
            "Batch 162 Loss: 0.2911\n",
            "Batch 163 Loss: 0.2469\n",
            "Batch 164 Loss: 0.2464\n",
            "Batch 165 Loss: 0.2553\n",
            "Batch 166 Loss: 0.7660\n",
            "Batch 167 Loss: 0.2963\n",
            "Batch 168 Loss: 0.2613\n",
            "Batch 169 Loss: 0.2240\n",
            "Batch 170 Loss: 0.4550\n",
            "Batch 171 Loss: 0.3195\n",
            "Batch 172 Loss: 0.4240\n",
            "Batch 173 Loss: 0.4054\n",
            "Batch 174 Loss: 0.4873\n",
            "Batch 175 Loss: 0.1415\n",
            "Batch 176 Loss: 0.5854\n",
            "Batch 177 Loss: 0.3702\n",
            "Batch 178 Loss: 0.4295\n",
            "Batch 179 Loss: 0.3671\n",
            "Batch 180 Loss: 0.8206\n",
            "Batch 181 Loss: 0.3398\n",
            "Batch 182 Loss: 0.2386\n",
            "Batch 183 Loss: 0.3628\n",
            "Batch 184 Loss: 0.2581\n",
            "Batch 185 Loss: 0.4833\n",
            "Batch 186 Loss: 0.3097\n",
            "Batch 187 Loss: 0.6443\n",
            "Batch 188 Loss: 0.4441\n",
            "Batch 189 Loss: 0.2644\n",
            "Batch 190 Loss: 0.2210\n",
            "Batch 191 Loss: 0.2440\n",
            "Batch 192 Loss: 0.6052\n",
            "Batch 193 Loss: 0.3516\n",
            "Batch 194 Loss: 0.4876\n",
            "Batch 195 Loss: 0.6763\n",
            "Batch 196 Loss: 0.4228\n",
            "Batch 197 Loss: 0.3079\n",
            "Batch 198 Loss: 0.2351\n",
            "Batch 199 Loss: 0.3327\n",
            "Batch 200 Loss: 0.3853\n",
            "Batch 201 Loss: 0.4979\n",
            "Batch 202 Loss: 0.3490\n",
            "Batch 203 Loss: 0.2493\n",
            "Batch 204 Loss: 0.4583\n",
            "Batch 205 Loss: 0.2689\n",
            "Batch 206 Loss: 0.1838\n",
            "Batch 207 Loss: 0.1284\n",
            "Batch 208 Loss: 0.1762\n",
            "Batch 209 Loss: 0.4338\n",
            "Batch 210 Loss: 0.2659\n",
            "Batch 211 Loss: 0.3863\n",
            "Batch 212 Loss: 0.2977\n",
            "Batch 213 Loss: 0.2064\n",
            "Batch 214 Loss: 0.4959\n",
            "Batch 215 Loss: 0.3845\n",
            "Batch 216 Loss: 0.3649\n",
            "Batch 217 Loss: 0.2233\n",
            "Batch 218 Loss: 0.1129\n",
            "Batch 219 Loss: 0.5830\n",
            "Batch 220 Loss: 0.1858\n",
            "Batch 221 Loss: 0.2753\n",
            "Batch 222 Loss: 0.2498\n",
            "Batch 223 Loss: 0.3350\n",
            "Batch 224 Loss: 0.1684\n",
            "Batch 225 Loss: 0.0629\n",
            "Batch 226 Loss: 0.4434\n",
            "Batch 227 Loss: 0.1290\n",
            "Batch 228 Loss: 0.7075\n",
            "Batch 229 Loss: 0.2880\n",
            "Batch 230 Loss: 0.3143\n",
            "Batch 231 Loss: 0.3273\n",
            "Batch 232 Loss: 0.3139\n",
            "Batch 233 Loss: 0.3097\n",
            "Batch 234 Loss: 0.2727\n",
            "Batch 235 Loss: 0.1209\n",
            "Batch 236 Loss: 0.3150\n",
            "Batch 237 Loss: 0.5888\n",
            "Batch 238 Loss: 0.0611\n",
            "Batch 239 Loss: 0.1541\n",
            "Batch 240 Loss: 0.2915\n",
            "Batch 241 Loss: 0.4487\n",
            "Batch 242 Loss: 0.1851\n",
            "Batch 243 Loss: 0.5165\n",
            "Batch 244 Loss: 0.4733\n",
            "Batch 245 Loss: 0.5940\n",
            "Batch 246 Loss: 0.3401\n",
            "Batch 247 Loss: 0.3137\n",
            "Batch 248 Loss: 0.4680\n",
            "Batch 249 Loss: 0.2935\n",
            "Batch 250 Loss: 0.1990\n",
            "Batch 251 Loss: 0.3970\n",
            "Batch 252 Loss: 0.4299\n",
            "Batch 253 Loss: 0.1450\n",
            "Batch 254 Loss: 0.1642\n",
            "Batch 255 Loss: 0.1427\n",
            "Batch 256 Loss: 0.5428\n",
            "Batch 257 Loss: 0.2356\n",
            "Batch 258 Loss: 0.2289\n",
            "Batch 259 Loss: 0.3304\n",
            "Batch 260 Loss: 0.4292\n",
            "Batch 261 Loss: 0.0947\n",
            "Batch 262 Loss: 0.4682\n",
            "Batch 263 Loss: 0.1446\n",
            "Batch 264 Loss: 0.2825\n",
            "Batch 265 Loss: 0.1353\n",
            "Batch 266 Loss: 0.2447\n",
            "Batch 267 Loss: 0.4283\n",
            "Batch 268 Loss: 0.1700\n",
            "Batch 269 Loss: 0.2102\n",
            "Batch 270 Loss: 0.4694\n",
            "Batch 271 Loss: 0.3360\n",
            "Batch 272 Loss: 0.3949\n",
            "Batch 273 Loss: 0.0590\n",
            "Batch 274 Loss: 0.5460\n",
            "Batch 275 Loss: 0.3597\n",
            "Batch 276 Loss: 0.4261\n",
            "Batch 277 Loss: 0.1946\n",
            "Batch 278 Loss: 0.1012\n",
            "Batch 279 Loss: 0.1200\n",
            "Batch 280 Loss: 0.2920\n",
            "Batch 281 Loss: 0.1956\n",
            "Batch 282 Loss: 0.1601\n",
            "Batch 283 Loss: 0.2410\n",
            "Batch 284 Loss: 0.3420\n",
            "Batch 285 Loss: 0.3550\n",
            "Batch 286 Loss: 0.1127\n",
            "Batch 287 Loss: 0.3036\n",
            "Batch 288 Loss: 0.1175\n",
            "Batch 289 Loss: 0.2274\n",
            "Batch 290 Loss: 0.1571\n",
            "Batch 291 Loss: 0.4408\n",
            "Batch 292 Loss: 0.2006\n",
            "Batch 293 Loss: 0.3362\n",
            "Batch 294 Loss: 0.2152\n",
            "Batch 295 Loss: 0.1414\n",
            "Batch 296 Loss: 0.2762\n",
            "Batch 297 Loss: 0.0831\n",
            "Batch 298 Loss: 0.1950\n",
            "Batch 299 Loss: 0.2783\n",
            "Batch 300 Loss: 0.4853\n",
            "Batch 301 Loss: 0.3805\n",
            "Batch 302 Loss: 0.1488\n",
            "Batch 303 Loss: 0.1529\n",
            "Batch 304 Loss: 0.2136\n",
            "Batch 305 Loss: 0.2711\n",
            "Batch 306 Loss: 0.3534\n",
            "Batch 307 Loss: 0.1816\n",
            "Batch 308 Loss: 0.2303\n",
            "Batch 309 Loss: 0.4629\n",
            "Batch 310 Loss: 0.2554\n",
            "Batch 311 Loss: 0.2027\n",
            "Batch 312 Loss: 0.6513\n",
            "Batch 313 Loss: 0.1705\n",
            "Batch 314 Loss: 0.1569\n",
            "Batch 315 Loss: 0.0837\n",
            "Batch 316 Loss: 0.1409\n",
            "Batch 317 Loss: 0.0710\n",
            "Batch 318 Loss: 0.1054\n",
            "Batch 319 Loss: 0.1363\n",
            "Batch 320 Loss: 0.5286\n",
            "Batch 321 Loss: 0.4371\n",
            "Batch 322 Loss: 0.4328\n",
            "Batch 323 Loss: 0.1830\n",
            "Batch 324 Loss: 0.2202\n",
            "Batch 325 Loss: 0.0898\n",
            "Batch 326 Loss: 0.1810\n",
            "Batch 327 Loss: 0.1330\n",
            "Batch 328 Loss: 0.1922\n",
            "Batch 329 Loss: 0.1653\n",
            "Batch 330 Loss: 0.3488\n",
            "Batch 331 Loss: 0.5947\n",
            "Batch 332 Loss: 0.1798\n",
            "Batch 333 Loss: 0.3173\n",
            "Batch 334 Loss: 0.0939\n",
            "Batch 335 Loss: 0.2716\n",
            "Batch 336 Loss: 0.1422\n",
            "Batch 337 Loss: 0.4550\n",
            "Batch 338 Loss: 0.4417\n",
            "Batch 339 Loss: 0.2913\n",
            "Batch 340 Loss: 0.2790\n",
            "Batch 341 Loss: 0.3767\n",
            "Batch 342 Loss: 0.3714\n",
            "Batch 343 Loss: 0.2703\n",
            "Batch 344 Loss: 0.3945\n",
            "Batch 345 Loss: 0.2617\n",
            "Batch 346 Loss: 0.5144\n",
            "Batch 347 Loss: 0.2457\n",
            "Batch 348 Loss: 0.1441\n",
            "Batch 349 Loss: 0.3801\n",
            "Batch 350 Loss: 0.2474\n",
            "Batch 351 Loss: 0.1614\n",
            "Batch 352 Loss: 0.1998\n",
            "Batch 353 Loss: 0.1185\n",
            "Batch 354 Loss: 0.2783\n",
            "Batch 355 Loss: 0.1678\n",
            "Batch 356 Loss: 0.2033\n",
            "Batch 357 Loss: 0.1786\n",
            "Batch 358 Loss: 0.2564\n",
            "Batch 359 Loss: 0.3091\n",
            "Batch 360 Loss: 0.1989\n",
            "Batch 361 Loss: 0.1668\n",
            "Batch 362 Loss: 0.4098\n",
            "Batch 363 Loss: 0.1297\n",
            "Batch 364 Loss: 0.0372\n",
            "Batch 365 Loss: 0.1746\n",
            "Batch 366 Loss: 0.1240\n",
            "Batch 367 Loss: 0.1327\n",
            "Batch 368 Loss: 0.1617\n",
            "Batch 369 Loss: 0.1630\n",
            "Batch 370 Loss: 0.0835\n",
            "Batch 371 Loss: 0.0505\n",
            "Batch 372 Loss: 0.1883\n",
            "Batch 373 Loss: 0.1286\n",
            "Batch 374 Loss: 0.1305\n",
            "Batch 375 Loss: 0.0864\n",
            "Batch 376 Loss: 0.0783\n",
            "Batch 377 Loss: 0.3249\n",
            "Batch 378 Loss: 0.1427\n",
            "Batch 379 Loss: 0.0796\n",
            "Batch 380 Loss: 0.1800\n",
            "Batch 381 Loss: 0.5778\n",
            "Batch 382 Loss: 0.1763\n",
            "Batch 383 Loss: 0.1383\n",
            "Batch 384 Loss: 0.3370\n",
            "Batch 385 Loss: 0.2737\n",
            "Batch 386 Loss: 0.2484\n",
            "Batch 387 Loss: 0.0751\n",
            "Batch 388 Loss: 0.3492\n",
            "Batch 389 Loss: 0.4544\n",
            "Batch 390 Loss: 0.1841\n",
            "Batch 391 Loss: 0.1656\n",
            "Batch 392 Loss: 0.4316\n",
            "Batch 393 Loss: 0.0511\n",
            "Batch 394 Loss: 0.2914\n",
            "Batch 395 Loss: 0.1368\n",
            "Batch 396 Loss: 0.0807\n",
            "Batch 397 Loss: 0.3844\n",
            "Batch 398 Loss: 0.1579\n",
            "Batch 399 Loss: 0.5051\n",
            "Batch 400 Loss: 0.3944\n",
            "Batch 401 Loss: 0.2339\n",
            "Batch 402 Loss: 0.4305\n",
            "Batch 403 Loss: 0.4660\n",
            "Batch 404 Loss: 0.4209\n",
            "Batch 405 Loss: 0.1651\n",
            "Batch 406 Loss: 0.0347\n",
            "Batch 407 Loss: 0.2977\n",
            "Batch 408 Loss: 0.4638\n",
            "Batch 409 Loss: 0.2572\n",
            "Batch 410 Loss: 0.2734\n",
            "Batch 411 Loss: 0.4065\n",
            "Batch 412 Loss: 0.1025\n",
            "Batch 413 Loss: 0.3018\n",
            "Batch 414 Loss: 0.2587\n",
            "Batch 415 Loss: 0.2369\n",
            "Batch 416 Loss: 0.2153\n",
            "Batch 417 Loss: 0.2667\n",
            "Batch 418 Loss: 0.1695\n",
            "Batch 419 Loss: 0.1250\n",
            "Batch 420 Loss: 0.0447\n",
            "Batch 421 Loss: 0.1727\n",
            "Batch 422 Loss: 0.3796\n",
            "Batch 423 Loss: 0.1743\n",
            "Batch 424 Loss: 0.1737\n",
            "Batch 425 Loss: 0.3703\n",
            "Batch 426 Loss: 0.2023\n",
            "Batch 427 Loss: 0.1440\n",
            "Batch 428 Loss: 0.6963\n",
            "Batch 429 Loss: 0.0342\n",
            "Batch 430 Loss: 0.2268\n",
            "Batch 431 Loss: 0.3024\n",
            "Batch 432 Loss: 0.1285\n",
            "Batch 433 Loss: 0.0883\n",
            "Batch 434 Loss: 0.1974\n",
            "Batch 435 Loss: 0.6014\n",
            "Batch 436 Loss: 0.1892\n",
            "Batch 437 Loss: 0.1493\n",
            "Batch 438 Loss: 0.0995\n",
            "Batch 439 Loss: 0.1383\n",
            "Batch 440 Loss: 0.2840\n",
            "Batch 441 Loss: 0.0832\n",
            "Batch 442 Loss: 0.2674\n",
            "Batch 443 Loss: 0.2057\n",
            "Batch 444 Loss: 0.1342\n",
            "Batch 445 Loss: 0.4662\n",
            "Batch 446 Loss: 0.1946\n",
            "Batch 447 Loss: 0.1617\n",
            "Batch 448 Loss: 0.2297\n",
            "Batch 449 Loss: 0.1360\n",
            "Batch 450 Loss: 0.3912\n",
            "Batch 451 Loss: 0.2737\n",
            "Batch 452 Loss: 0.2123\n",
            "Batch 453 Loss: 0.1236\n",
            "Batch 454 Loss: 0.2869\n",
            "Batch 455 Loss: 0.1613\n",
            "Batch 456 Loss: 0.5586\n",
            "Batch 457 Loss: 0.0963\n",
            "Batch 458 Loss: 0.1737\n",
            "Batch 459 Loss: 0.2266\n",
            "Batch 460 Loss: 0.1891\n",
            "Batch 461 Loss: 0.2463\n",
            "Batch 462 Loss: 0.4855\n",
            "Batch 463 Loss: 0.5057\n",
            "Batch 464 Loss: 0.2701\n",
            "Batch 465 Loss: 0.1250\n",
            "Batch 466 Loss: 0.6658\n",
            "Batch 467 Loss: 0.1694\n",
            "Batch 468 Loss: 0.3224\n",
            "Batch 469 Loss: 0.3780\n",
            "Batch 470 Loss: 0.1246\n",
            "Batch 471 Loss: 0.1274\n",
            "Batch 472 Loss: 0.2501\n",
            "Batch 473 Loss: 0.2157\n",
            "Batch 474 Loss: 0.2655\n",
            "Batch 475 Loss: 0.1626\n",
            "Batch 476 Loss: 0.1612\n",
            "Batch 477 Loss: 0.1408\n",
            "Batch 478 Loss: 0.3425\n",
            "Batch 479 Loss: 0.1321\n",
            "Batch 480 Loss: 0.1332\n",
            "Batch 481 Loss: 0.3260\n",
            "Batch 482 Loss: 0.0869\n",
            "Batch 483 Loss: 0.3113\n",
            "Batch 484 Loss: 0.2468\n",
            "Batch 485 Loss: 0.3190\n",
            "Batch 486 Loss: 0.2858\n",
            "Batch 487 Loss: 0.1978\n",
            "Batch 488 Loss: 0.2998\n",
            "Batch 489 Loss: 0.1396\n",
            "Batch 490 Loss: 0.2640\n",
            "Batch 491 Loss: 0.4282\n",
            "Batch 492 Loss: 0.2656\n",
            "Batch 493 Loss: 0.1131\n",
            "Batch 494 Loss: 0.1506\n",
            "Batch 495 Loss: 0.1768\n",
            "Batch 496 Loss: 0.1554\n",
            "Batch 497 Loss: 0.2234\n",
            "Batch 498 Loss: 0.1856\n",
            "Batch 499 Loss: 0.3333\n",
            "Batch 500 Loss: 0.2818\n",
            "Batch 501 Loss: 0.1866\n",
            "Batch 502 Loss: 0.1879\n",
            "Batch 503 Loss: 0.1731\n",
            "Batch 504 Loss: 0.2144\n",
            "Batch 505 Loss: 0.4269\n",
            "Batch 506 Loss: 0.4005\n",
            "Batch 507 Loss: 0.1090\n",
            "Batch 508 Loss: 0.2496\n",
            "Batch 509 Loss: 0.2396\n",
            "Batch 510 Loss: 0.2162\n",
            "Batch 511 Loss: 0.1977\n",
            "Batch 512 Loss: 0.1747\n",
            "Batch 513 Loss: 0.0865\n",
            "Batch 514 Loss: 0.0863\n",
            "Batch 515 Loss: 0.2363\n",
            "Batch 516 Loss: 0.0932\n",
            "Batch 517 Loss: 0.2074\n",
            "Batch 518 Loss: 0.2012\n",
            "Batch 519 Loss: 0.0797\n",
            "Batch 520 Loss: 0.2238\n",
            "Batch 521 Loss: 0.1470\n",
            "Batch 522 Loss: 0.2994\n",
            "Batch 523 Loss: 0.0851\n",
            "Batch 524 Loss: 0.2318\n",
            "Batch 525 Loss: 0.2337\n",
            "Batch 526 Loss: 0.1624\n",
            "Batch 527 Loss: 0.0909\n",
            "Batch 528 Loss: 0.3247\n",
            "Batch 529 Loss: 0.3432\n",
            "Batch 530 Loss: 0.0417\n",
            "Batch 531 Loss: 0.3199\n",
            "Batch 532 Loss: 0.1485\n",
            "Batch 533 Loss: 0.2920\n",
            "Batch 534 Loss: 0.0591\n",
            "Batch 535 Loss: 0.1193\n",
            "Batch 536 Loss: 0.3894\n",
            "Batch 537 Loss: 0.2134\n",
            "Batch 538 Loss: 0.2857\n",
            "Batch 539 Loss: 0.2664\n",
            "Batch 540 Loss: 0.3058\n",
            "Batch 541 Loss: 0.0687\n",
            "Batch 542 Loss: 0.2443\n",
            "Batch 543 Loss: 0.1412\n",
            "Batch 544 Loss: 0.2734\n",
            "Batch 545 Loss: 0.0416\n",
            "Batch 546 Loss: 0.1364\n",
            "Batch 547 Loss: 0.2230\n",
            "Batch 548 Loss: 0.0296\n",
            "Batch 549 Loss: 0.1782\n",
            "Batch 550 Loss: 0.2112\n",
            "Batch 551 Loss: 0.0300\n",
            "Batch 552 Loss: 0.2397\n",
            "Batch 553 Loss: 0.0882\n",
            "Batch 554 Loss: 0.3673\n",
            "Batch 555 Loss: 0.2140\n",
            "Batch 556 Loss: 0.0856\n",
            "Batch 557 Loss: 0.2292\n",
            "Batch 558 Loss: 0.1067\n",
            "Batch 559 Loss: 0.0547\n",
            "Batch 560 Loss: 0.0224\n",
            "Batch 561 Loss: 0.0772\n",
            "Batch 562 Loss: 0.1526\n",
            "Batch 563 Loss: 0.1578\n",
            "Batch 564 Loss: 0.0285\n",
            "Batch 565 Loss: 0.0737\n",
            "Batch 566 Loss: 0.0571\n",
            "Batch 567 Loss: 0.0627\n",
            "Batch 568 Loss: 0.1254\n",
            "Batch 569 Loss: 0.2179\n",
            "Batch 570 Loss: 0.2860\n",
            "Batch 571 Loss: 0.1499\n",
            "Batch 572 Loss: 0.0506\n",
            "Batch 573 Loss: 0.0155\n",
            "Batch 574 Loss: 0.2266\n",
            "Batch 575 Loss: 0.2228\n",
            "Batch 576 Loss: 0.1675\n",
            "Batch 577 Loss: 0.1588\n",
            "Batch 578 Loss: 0.1382\n",
            "Batch 579 Loss: 0.0558\n",
            "Batch 580 Loss: 0.4039\n",
            "Batch 581 Loss: 0.0758\n",
            "Batch 582 Loss: 0.3408\n",
            "Batch 583 Loss: 0.0856\n",
            "Batch 584 Loss: 0.0963\n",
            "Batch 585 Loss: 0.2035\n",
            "Batch 586 Loss: 0.0863\n",
            "Batch 587 Loss: 0.0795\n",
            "Batch 588 Loss: 0.1117\n",
            "Batch 589 Loss: 0.2873\n",
            "Batch 590 Loss: 0.0992\n",
            "Batch 591 Loss: 0.0927\n",
            "Batch 592 Loss: 0.2372\n",
            "Batch 593 Loss: 0.1894\n",
            "Batch 594 Loss: 0.1090\n",
            "Batch 595 Loss: 0.4974\n",
            "Batch 596 Loss: 0.1616\n",
            "Batch 597 Loss: 0.4841\n",
            "Batch 598 Loss: 0.3235\n",
            "Batch 599 Loss: 0.2202\n",
            "Batch 600 Loss: 0.2190\n",
            "Batch 601 Loss: 0.0553\n",
            "Batch 602 Loss: 0.0940\n",
            "Batch 603 Loss: 0.2910\n",
            "Batch 604 Loss: 0.3312\n",
            "Batch 605 Loss: 0.2013\n",
            "Batch 606 Loss: 0.2074\n",
            "Batch 607 Loss: 0.2780\n",
            "Batch 608 Loss: 0.2281\n",
            "Batch 609 Loss: 0.1094\n",
            "Batch 610 Loss: 0.0982\n",
            "Batch 611 Loss: 0.2975\n",
            "Batch 612 Loss: 0.0571\n",
            "Batch 613 Loss: 0.0193\n",
            "Batch 614 Loss: 0.2972\n",
            "Batch 615 Loss: 0.1647\n",
            "Batch 616 Loss: 0.1506\n",
            "Batch 617 Loss: 0.0644\n",
            "Batch 618 Loss: 0.0421\n",
            "Batch 619 Loss: 0.0337\n",
            "Batch 620 Loss: 0.2988\n",
            "Batch 621 Loss: 0.0971\n",
            "Batch 622 Loss: 0.1967\n",
            "Batch 623 Loss: 0.1307\n",
            "Batch 624 Loss: 0.0392\n",
            "Batch 625 Loss: 0.1159\n",
            "Batch 626 Loss: 0.2141\n",
            "Batch 627 Loss: 0.1474\n",
            "Batch 628 Loss: 0.0307\n",
            "Batch 629 Loss: 0.0725\n",
            "Batch 630 Loss: 0.2111\n",
            "Batch 631 Loss: 0.0524\n",
            "Batch 632 Loss: 0.2984\n",
            "Batch 633 Loss: 0.2491\n",
            "Batch 634 Loss: 0.1375\n",
            "Batch 635 Loss: 0.1182\n",
            "Batch 636 Loss: 0.1879\n",
            "Batch 637 Loss: 0.3309\n",
            "Batch 638 Loss: 0.0640\n",
            "Batch 639 Loss: 0.0483\n",
            "Batch 640 Loss: 0.1752\n",
            "Batch 641 Loss: 0.0200\n",
            "Batch 642 Loss: 0.2755\n",
            "Batch 643 Loss: 0.3288\n",
            "Batch 644 Loss: 0.0683\n",
            "Batch 645 Loss: 0.2666\n",
            "Batch 646 Loss: 0.2178\n",
            "Batch 647 Loss: 0.1231\n",
            "Batch 648 Loss: 0.4428\n",
            "Batch 649 Loss: 0.1912\n",
            "Batch 650 Loss: 0.1160\n",
            "Batch 651 Loss: 0.0499\n",
            "Batch 652 Loss: 0.1905\n",
            "Batch 653 Loss: 0.4807\n",
            "Batch 654 Loss: 0.1151\n",
            "Batch 655 Loss: 0.0691\n",
            "Batch 656 Loss: 0.0725\n",
            "Batch 657 Loss: 0.1289\n",
            "Batch 658 Loss: 0.0091\n",
            "Batch 659 Loss: 0.1375\n",
            "Batch 660 Loss: 0.0801\n",
            "Batch 661 Loss: 0.1953\n",
            "Batch 662 Loss: 0.0646\n",
            "Batch 663 Loss: 0.1859\n",
            "Batch 664 Loss: 0.2365\n",
            "Batch 665 Loss: 0.0567\n",
            "Batch 666 Loss: 0.0929\n",
            "Batch 667 Loss: 0.1111\n",
            "Batch 668 Loss: 0.1561\n",
            "Batch 669 Loss: 0.1991\n",
            "Batch 670 Loss: 0.2289\n",
            "Batch 671 Loss: 0.9860\n",
            "Batch 672 Loss: 0.4042\n",
            "Batch 673 Loss: 0.1957\n",
            "Batch 674 Loss: 0.0788\n",
            "Batch 675 Loss: 0.2059\n",
            "Batch 676 Loss: 0.0952\n",
            "Batch 677 Loss: 0.0731\n",
            "Batch 678 Loss: 0.0546\n",
            "Batch 679 Loss: 0.2366\n",
            "Batch 680 Loss: 0.2169\n",
            "Batch 681 Loss: 0.2203\n",
            "Batch 682 Loss: 0.2308\n",
            "Batch 683 Loss: 0.0674\n",
            "Batch 684 Loss: 0.1360\n",
            "Batch 685 Loss: 0.2453\n",
            "Batch 686 Loss: 0.1355\n",
            "Batch 687 Loss: 0.0277\n",
            "Batch 688 Loss: 0.0352\n",
            "Batch 689 Loss: 0.1016\n",
            "Batch 690 Loss: 0.2258\n",
            "Batch 691 Loss: 0.2191\n",
            "Batch 692 Loss: 0.2338\n",
            "Batch 693 Loss: 0.2248\n",
            "Batch 694 Loss: 0.3039\n",
            "Batch 695 Loss: 0.4016\n",
            "Batch 696 Loss: 0.0434\n",
            "Batch 697 Loss: 0.4019\n",
            "Batch 698 Loss: 0.3164\n",
            "Batch 699 Loss: 0.1488\n",
            "Batch 700 Loss: 0.6885\n",
            "Batch 701 Loss: 0.2331\n",
            "Batch 702 Loss: 0.1136\n",
            "Batch 703 Loss: 0.3856\n",
            "Batch 704 Loss: 0.1155\n",
            "Batch 705 Loss: 0.2156\n",
            "Batch 706 Loss: 0.1570\n",
            "Batch 707 Loss: 0.3548\n",
            "Batch 708 Loss: 0.4943\n",
            "Batch 709 Loss: 0.2921\n",
            "Batch 710 Loss: 0.2140\n",
            "Batch 711 Loss: 0.4951\n",
            "Batch 712 Loss: 0.3605\n",
            "Batch 713 Loss: 0.1557\n",
            "Batch 714 Loss: 0.2582\n",
            "Batch 715 Loss: 0.2822\n",
            "Batch 716 Loss: 0.0636\n",
            "Batch 717 Loss: 0.1440\n",
            "Batch 718 Loss: 0.1704\n",
            "Batch 719 Loss: 0.3244\n",
            "Batch 720 Loss: 0.2666\n",
            "Batch 721 Loss: 0.1221\n",
            "Batch 722 Loss: 0.1093\n",
            "Batch 723 Loss: 0.2612\n",
            "Batch 724 Loss: 0.2702\n",
            "Batch 725 Loss: 0.2399\n",
            "Batch 726 Loss: 0.1552\n",
            "Batch 727 Loss: 0.1130\n",
            "Batch 728 Loss: 0.2109\n",
            "Batch 729 Loss: 0.0938\n",
            "Batch 730 Loss: 0.1726\n",
            "Batch 731 Loss: 0.1926\n",
            "Batch 732 Loss: 0.1689\n",
            "Batch 733 Loss: 0.1387\n",
            "Batch 734 Loss: 0.1763\n",
            "Batch 735 Loss: 0.1007\n",
            "Batch 736 Loss: 0.0974\n",
            "Batch 737 Loss: 0.2985\n",
            "Batch 738 Loss: 0.1540\n",
            "Batch 739 Loss: 0.3414\n",
            "Batch 740 Loss: 0.2255\n",
            "Batch 741 Loss: 0.2893\n",
            "Batch 742 Loss: 0.2771\n",
            "Batch 743 Loss: 0.1952\n",
            "Batch 744 Loss: 0.1600\n",
            "Batch 745 Loss: 0.0348\n",
            "Batch 746 Loss: 0.4006\n",
            "Batch 747 Loss: 0.0931\n",
            "Batch 748 Loss: 0.2587\n",
            "Batch 749 Loss: 0.1440\n",
            "Batch 750 Loss: 0.2528\n",
            "Batch 751 Loss: 0.1850\n",
            "Batch 752 Loss: 0.1403\n",
            "Batch 753 Loss: 0.1476\n",
            "Batch 754 Loss: 0.1158\n",
            "Batch 755 Loss: 0.2744\n",
            "Batch 756 Loss: 0.1805\n",
            "Batch 757 Loss: 0.2789\n",
            "Batch 758 Loss: 0.1963\n",
            "Batch 759 Loss: 0.1527\n",
            "Batch 760 Loss: 0.0977\n",
            "Batch 761 Loss: 0.2044\n",
            "Batch 762 Loss: 0.0541\n",
            "Batch 763 Loss: 0.1261\n",
            "Batch 764 Loss: 0.1389\n",
            "Batch 765 Loss: 0.0895\n",
            "Batch 766 Loss: 0.1013\n",
            "Batch 767 Loss: 0.1318\n",
            "Batch 768 Loss: 0.1626\n",
            "Batch 769 Loss: 0.1809\n",
            "Batch 770 Loss: 0.3036\n",
            "Batch 771 Loss: 0.0989\n",
            "Batch 772 Loss: 0.2155\n",
            "Batch 773 Loss: 0.0269\n",
            "Batch 774 Loss: 0.0233\n",
            "Batch 775 Loss: 0.2467\n",
            "Batch 776 Loss: 0.2405\n",
            "Batch 777 Loss: 0.2046\n",
            "Batch 778 Loss: 0.2840\n",
            "Batch 779 Loss: 0.0810\n",
            "Batch 780 Loss: 0.2294\n",
            "Batch 781 Loss: 0.0547\n",
            "Batch 782 Loss: 0.1292\n",
            "Batch 783 Loss: 0.2867\n",
            "Batch 784 Loss: 0.1338\n",
            "Batch 785 Loss: 0.2593\n",
            "Batch 786 Loss: 0.0637\n",
            "Batch 787 Loss: 0.0341\n",
            "Batch 788 Loss: 0.0593\n",
            "Batch 789 Loss: 0.0491\n",
            "Batch 790 Loss: 0.1007\n",
            "Batch 791 Loss: 0.3079\n",
            "Batch 792 Loss: 0.5382\n",
            "Batch 793 Loss: 0.3569\n",
            "Batch 794 Loss: 0.1476\n",
            "Batch 795 Loss: 0.1660\n",
            "Batch 796 Loss: 0.3667\n",
            "Batch 797 Loss: 0.0881\n",
            "Batch 798 Loss: 0.1703\n",
            "Batch 799 Loss: 0.2161\n",
            "Batch 800 Loss: 0.4176\n",
            "Batch 801 Loss: 0.0840\n",
            "Batch 802 Loss: 0.1173\n",
            "Batch 803 Loss: 0.4032\n",
            "Batch 804 Loss: 0.0970\n",
            "Batch 805 Loss: 0.2227\n",
            "Batch 806 Loss: 0.2207\n",
            "Batch 807 Loss: 0.1123\n",
            "Batch 808 Loss: 0.4341\n",
            "Batch 809 Loss: 0.1984\n",
            "Batch 810 Loss: 0.1060\n",
            "Batch 811 Loss: 0.0466\n",
            "Batch 812 Loss: 0.0914\n",
            "Batch 813 Loss: 0.1057\n",
            "Batch 814 Loss: 0.1154\n",
            "Batch 815 Loss: 0.2459\n",
            "Batch 816 Loss: 0.1599\n",
            "Batch 817 Loss: 0.2989\n",
            "Batch 818 Loss: 0.3449\n",
            "Batch 819 Loss: 0.0599\n",
            "Batch 820 Loss: 0.4079\n",
            "Batch 821 Loss: 0.1807\n",
            "Batch 822 Loss: 0.1717\n",
            "Batch 823 Loss: 0.0416\n",
            "Batch 824 Loss: 0.5628\n",
            "Batch 825 Loss: 0.1292\n",
            "Batch 826 Loss: 0.1479\n",
            "Batch 827 Loss: 0.1496\n",
            "Batch 828 Loss: 0.0760\n",
            "Batch 829 Loss: 0.1476\n",
            "Batch 830 Loss: 0.3029\n",
            "Batch 831 Loss: 0.2638\n",
            "Batch 832 Loss: 0.1127\n",
            "Batch 833 Loss: 0.0692\n",
            "Batch 834 Loss: 0.2406\n",
            "Batch 835 Loss: 0.1250\n",
            "Batch 836 Loss: 0.4214\n",
            "Batch 837 Loss: 0.1903\n",
            "Batch 838 Loss: 0.0845\n",
            "Batch 839 Loss: 0.0357\n",
            "Batch 840 Loss: 0.1792\n",
            "Batch 841 Loss: 0.0781\n",
            "Batch 842 Loss: 0.2097\n",
            "Batch 843 Loss: 0.1584\n",
            "Batch 844 Loss: 0.1883\n",
            "Batch 845 Loss: 0.4095\n",
            "Batch 846 Loss: 0.2251\n",
            "Batch 847 Loss: 0.0806\n",
            "Batch 848 Loss: 0.0560\n",
            "Batch 849 Loss: 0.1140\n",
            "Batch 850 Loss: 0.1404\n",
            "Batch 851 Loss: 0.3144\n",
            "Batch 852 Loss: 0.0626\n",
            "Batch 853 Loss: 0.1200\n",
            "Batch 854 Loss: 0.1055\n",
            "Batch 855 Loss: 0.0484\n",
            "Batch 856 Loss: 0.0634\n",
            "Batch 857 Loss: 0.1310\n",
            "Batch 858 Loss: 0.0519\n",
            "Batch 859 Loss: 0.0599\n",
            "Batch 860 Loss: 0.1767\n",
            "Batch 861 Loss: 0.1723\n",
            "Batch 862 Loss: 0.1933\n",
            "Batch 863 Loss: 0.0496\n",
            "Batch 864 Loss: 0.3764\n",
            "Batch 865 Loss: 0.0501\n",
            "Batch 866 Loss: 0.0774\n",
            "Batch 867 Loss: 0.1648\n",
            "Batch 868 Loss: 0.1393\n",
            "Batch 869 Loss: 0.1890\n",
            "Batch 870 Loss: 0.0589\n",
            "Batch 871 Loss: 0.0713\n",
            "Batch 872 Loss: 0.1070\n",
            "Batch 873 Loss: 0.2613\n",
            "Batch 874 Loss: 0.1443\n",
            "Batch 875 Loss: 0.0857\n",
            "Batch 876 Loss: 0.3161\n",
            "Batch 877 Loss: 0.0622\n",
            "Batch 878 Loss: 0.1852\n",
            "Batch 879 Loss: 0.2312\n",
            "Batch 880 Loss: 0.2515\n",
            "Batch 881 Loss: 0.0442\n",
            "Batch 882 Loss: 0.0430\n",
            "Batch 883 Loss: 0.0054\n",
            "Batch 884 Loss: 0.1183\n",
            "Batch 885 Loss: 0.1874\n",
            "Batch 886 Loss: 0.0461\n",
            "Batch 887 Loss: 0.3533\n",
            "Batch 888 Loss: 0.2762\n",
            "Batch 889 Loss: 0.3977\n",
            "Batch 890 Loss: 0.2399\n",
            "Batch 891 Loss: 0.1300\n",
            "Batch 892 Loss: 0.2402\n",
            "Batch 893 Loss: 0.0103\n",
            "Batch 894 Loss: 0.1225\n",
            "Batch 895 Loss: 0.2046\n",
            "Batch 896 Loss: 0.1171\n",
            "Batch 897 Loss: 0.0643\n",
            "Batch 898 Loss: 0.1110\n",
            "Batch 899 Loss: 0.1625\n",
            "Batch 900 Loss: 0.0976\n",
            "Batch 901 Loss: 0.2596\n",
            "Batch 902 Loss: 0.0602\n",
            "Batch 903 Loss: 0.0544\n",
            "Batch 904 Loss: 0.1292\n",
            "Batch 905 Loss: 0.1142\n",
            "Batch 906 Loss: 0.0312\n",
            "Batch 907 Loss: 0.0972\n",
            "Batch 908 Loss: 0.1463\n",
            "Batch 909 Loss: 0.2956\n",
            "Batch 910 Loss: 0.0394\n",
            "Batch 911 Loss: 0.0289\n",
            "Batch 912 Loss: 0.1026\n",
            "Batch 913 Loss: 0.0452\n",
            "Batch 914 Loss: 0.0400\n",
            "Batch 915 Loss: 0.1423\n",
            "Batch 916 Loss: 0.3471\n",
            "Batch 917 Loss: 0.1824\n",
            "Batch 918 Loss: 0.0814\n",
            "Batch 919 Loss: 0.0621\n",
            "Batch 920 Loss: 0.0743\n",
            "Batch 921 Loss: 0.0873\n",
            "Batch 922 Loss: 0.1696\n",
            "Batch 923 Loss: 0.3925\n",
            "Batch 924 Loss: 0.3535\n",
            "Batch 925 Loss: 0.3421\n",
            "Batch 926 Loss: 0.1072\n",
            "Batch 927 Loss: 0.2732\n",
            "Batch 928 Loss: 0.5166\n",
            "Batch 929 Loss: 0.1338\n",
            "Batch 930 Loss: 0.0691\n",
            "Batch 931 Loss: 0.1570\n",
            "Batch 932 Loss: 0.1906\n",
            "Batch 933 Loss: 0.0488\n",
            "Batch 934 Loss: 0.0860\n",
            "Batch 935 Loss: 0.2532\n",
            "Batch 936 Loss: 0.0287\n",
            "Batch 937 Loss: 0.1944\n",
            "Batch 938 Loss: 0.1202\n",
            "Batch 939 Loss: 0.0745\n",
            "Batch 940 Loss: 0.2207\n",
            "Batch 941 Loss: 0.0949\n",
            "Batch 942 Loss: 0.0515\n",
            "Batch 943 Loss: 0.1554\n",
            "Batch 944 Loss: 0.2760\n",
            "Batch 945 Loss: 0.2573\n",
            "Batch 946 Loss: 0.0630\n",
            "Batch 947 Loss: 0.0925\n",
            "Batch 948 Loss: 0.1926\n",
            "Batch 949 Loss: 0.0673\n",
            "Batch 950 Loss: 0.1232\n",
            "Batch 951 Loss: 0.1088\n",
            "Batch 952 Loss: 0.3016\n",
            "Batch 953 Loss: 0.0460\n",
            "Batch 954 Loss: 0.1901\n",
            "Batch 955 Loss: 0.0978\n",
            "Batch 956 Loss: 0.0312\n",
            "Batch 957 Loss: 0.0603\n",
            "Batch 958 Loss: 0.1955\n",
            "Batch 959 Loss: 0.0181\n",
            "Batch 960 Loss: 0.1447\n",
            "Batch 961 Loss: 0.3395\n",
            "Batch 962 Loss: 0.0836\n",
            "Batch 963 Loss: 0.0735\n",
            "Batch 964 Loss: 0.2117\n",
            "Batch 965 Loss: 0.4207\n",
            "Batch 966 Loss: 0.0527\n",
            "Batch 967 Loss: 0.1694\n",
            "Batch 968 Loss: 0.3097\n",
            "Batch 969 Loss: 0.0762\n",
            "Batch 970 Loss: 0.1668\n",
            "Batch 971 Loss: 0.2090\n",
            "Batch 972 Loss: 0.0785\n",
            "Batch 973 Loss: 0.1294\n",
            "Batch 974 Loss: 0.1611\n",
            "Batch 975 Loss: 0.0949\n",
            "Batch 976 Loss: 0.0909\n",
            "Batch 977 Loss: 0.4247\n",
            "Batch 978 Loss: 0.0852\n",
            "Batch 979 Loss: 0.1320\n",
            "Batch 980 Loss: 0.1040\n",
            "Batch 981 Loss: 0.0944\n",
            "Batch 982 Loss: 0.0950\n",
            "Batch 983 Loss: 0.0131\n",
            "Batch 984 Loss: 0.0395\n",
            "Batch 985 Loss: 0.0756\n",
            "Batch 986 Loss: 0.0194\n",
            "Batch 987 Loss: 0.2912\n",
            "Batch 988 Loss: 0.0589\n",
            "Batch 989 Loss: 0.0589\n",
            "Batch 990 Loss: 0.1869\n",
            "Batch 991 Loss: 0.2078\n",
            "Batch 992 Loss: 0.0569\n",
            "Batch 993 Loss: 0.0959\n",
            "Batch 994 Loss: 0.1067\n",
            "Batch 995 Loss: 0.1510\n",
            "Batch 996 Loss: 0.0350\n",
            "Batch 997 Loss: 0.0086\n",
            "Batch 998 Loss: 0.0295\n",
            "Batch 999 Loss: 0.7070\n",
            "Batch 1000 Loss: 0.1781\n",
            "Batch 1001 Loss: 0.0669\n",
            "Batch 1002 Loss: 0.5335\n",
            "Batch 1003 Loss: 0.0354\n",
            "Batch 1004 Loss: 0.1499\n",
            "Batch 1005 Loss: 0.0668\n",
            "Batch 1006 Loss: 0.2085\n",
            "Batch 1007 Loss: 0.0625\n",
            "Batch 1008 Loss: 0.0465\n",
            "Batch 1009 Loss: 0.1217\n",
            "Batch 1010 Loss: 0.1009\n",
            "Batch 1011 Loss: 0.1407\n",
            "Batch 1012 Loss: 0.2619\n",
            "Batch 1013 Loss: 0.0709\n",
            "Batch 1014 Loss: 0.1818\n",
            "Batch 1015 Loss: 0.0933\n",
            "Batch 1016 Loss: 0.1091\n",
            "Batch 1017 Loss: 0.1633\n",
            "Batch 1018 Loss: 0.0730\n",
            "Batch 1019 Loss: 0.2969\n",
            "Batch 1020 Loss: 0.1082\n",
            "Batch 1021 Loss: 0.0916\n",
            "Batch 1022 Loss: 0.2907\n",
            "Batch 1023 Loss: 0.1442\n",
            "Batch 1024 Loss: 0.1519\n",
            "Batch 1025 Loss: 0.3291\n",
            "Batch 1026 Loss: 0.0200\n",
            "Batch 1027 Loss: 0.1384\n",
            "Batch 1028 Loss: 0.1073\n",
            "Batch 1029 Loss: 0.5277\n",
            "Batch 1030 Loss: 0.1162\n",
            "Batch 1031 Loss: 0.0721\n",
            "Batch 1032 Loss: 0.0118\n",
            "Batch 1033 Loss: 0.1245\n",
            "Batch 1034 Loss: 0.1129\n",
            "Batch 1035 Loss: 0.1854\n",
            "Batch 1036 Loss: 0.2210\n",
            "Batch 1037 Loss: 0.2568\n",
            "Batch 1038 Loss: 0.1748\n",
            "Batch 1039 Loss: 0.3935\n",
            "Batch 1040 Loss: 0.1693\n",
            "Batch 1041 Loss: 0.1608\n",
            "Batch 1042 Loss: 0.1690\n",
            "Batch 1043 Loss: 0.0987\n",
            "Batch 1044 Loss: 0.2592\n",
            "Batch 1045 Loss: 0.1308\n",
            "Batch 1046 Loss: 0.1269\n",
            "Batch 1047 Loss: 0.1851\n",
            "Batch 1048 Loss: 0.2358\n",
            "Batch 1049 Loss: 0.1025\n",
            "Batch 1050 Loss: 0.1210\n",
            "Batch 1051 Loss: 0.0898\n",
            "Batch 1052 Loss: 0.2933\n",
            "Batch 1053 Loss: 0.0182\n",
            "Batch 1054 Loss: 0.0463\n",
            "Batch 1055 Loss: 0.0999\n",
            "Batch 1056 Loss: 0.0995\n",
            "Batch 1057 Loss: 0.0792\n",
            "Batch 1058 Loss: 0.0431\n",
            "Batch 1059 Loss: 0.1679\n",
            "Batch 1060 Loss: 0.2004\n",
            "Batch 1061 Loss: 0.0263\n",
            "Batch 1062 Loss: 0.0459\n",
            "Batch 1063 Loss: 0.0346\n",
            "Batch 1064 Loss: 0.1001\n",
            "Batch 1065 Loss: 0.0208\n",
            "Batch 1066 Loss: 0.1378\n",
            "Batch 1067 Loss: 0.1458\n",
            "Batch 1068 Loss: 0.0636\n",
            "Batch 1069 Loss: 0.0487\n",
            "Batch 1070 Loss: 0.0962\n",
            "Batch 1071 Loss: 0.0202\n",
            "Batch 1072 Loss: 0.0868\n",
            "Batch 1073 Loss: 0.2224\n",
            "Batch 1074 Loss: 0.0707\n",
            "Batch 1075 Loss: 0.0888\n",
            "Batch 1076 Loss: 0.1732\n",
            "Batch 1077 Loss: 0.1112\n",
            "Batch 1078 Loss: 0.0976\n",
            "Batch 1079 Loss: 0.0346\n",
            "Batch 1080 Loss: 0.0611\n",
            "Batch 1081 Loss: 0.1326\n",
            "Batch 1082 Loss: 0.0167\n",
            "Batch 1083 Loss: 0.0153\n",
            "Batch 1084 Loss: 0.1227\n",
            "Batch 1085 Loss: 0.1712\n",
            "Batch 1086 Loss: 0.0564\n",
            "Batch 1087 Loss: 0.0386\n",
            "Batch 1088 Loss: 0.2234\n",
            "Batch 1089 Loss: 0.0775\n",
            "Batch 1090 Loss: 0.2411\n",
            "Batch 1091 Loss: 0.1808\n",
            "Batch 1092 Loss: 0.0958\n",
            "Batch 1093 Loss: 0.1045\n",
            "Batch 1094 Loss: 0.1169\n",
            "Batch 1095 Loss: 0.0887\n",
            "Batch 1096 Loss: 0.0556\n",
            "Batch 1097 Loss: 0.2197\n",
            "Batch 1098 Loss: 0.2522\n",
            "Batch 1099 Loss: 0.0209\n",
            "Batch 1100 Loss: 0.0952\n",
            "Batch 1101 Loss: 0.2204\n",
            "Batch 1102 Loss: 0.0470\n",
            "Batch 1103 Loss: 0.1087\n",
            "Batch 1104 Loss: 0.1943\n",
            "Batch 1105 Loss: 0.0748\n",
            "Batch 1106 Loss: 0.1826\n",
            "Batch 1107 Loss: 0.3084\n",
            "Batch 1108 Loss: 0.2192\n",
            "Batch 1109 Loss: 0.3056\n",
            "Batch 1110 Loss: 0.0160\n",
            "Batch 1111 Loss: 0.1157\n",
            "Batch 1112 Loss: 0.1829\n",
            "Batch 1113 Loss: 0.0709\n",
            "Batch 1114 Loss: 0.0858\n",
            "Batch 1115 Loss: 0.0527\n",
            "Batch 1116 Loss: 0.1098\n",
            "Batch 1117 Loss: 0.0041\n",
            "Batch 1118 Loss: 0.2135\n",
            "Batch 1119 Loss: 0.0995\n",
            "Batch 1120 Loss: 0.1748\n",
            "Batch 1121 Loss: 0.4487\n",
            "Batch 1122 Loss: 0.2406\n",
            "Batch 1123 Loss: 0.1275\n",
            "Batch 1124 Loss: 0.0070\n",
            "Batch 1125 Loss: 0.4343\n",
            "Batch 1126 Loss: 0.0914\n",
            "Batch 1127 Loss: 0.1379\n",
            "Batch 1128 Loss: 0.0153\n",
            "Batch 1129 Loss: 0.3394\n",
            "Batch 1130 Loss: 0.1146\n",
            "Batch 1131 Loss: 0.0396\n",
            "Batch 1132 Loss: 0.0843\n",
            "Batch 1133 Loss: 0.1994\n",
            "Batch 1134 Loss: 0.0463\n",
            "Batch 1135 Loss: 0.0481\n",
            "Batch 1136 Loss: 0.2327\n",
            "Batch 1137 Loss: 0.0539\n",
            "Batch 1138 Loss: 0.1821\n",
            "Batch 1139 Loss: 0.3299\n",
            "Batch 1140 Loss: 0.1093\n",
            "Batch 1141 Loss: 0.0661\n",
            "Batch 1142 Loss: 0.1032\n",
            "Batch 1143 Loss: 0.1482\n",
            "Batch 1144 Loss: 0.0469\n",
            "Batch 1145 Loss: 0.1570\n",
            "Batch 1146 Loss: 0.0840\n",
            "Batch 1147 Loss: 0.1587\n",
            "Batch 1148 Loss: 0.3098\n",
            "Batch 1149 Loss: 0.3355\n",
            "Batch 1150 Loss: 0.5221\n",
            "Batch 1151 Loss: 0.0131\n",
            "Batch 1152 Loss: 0.0951\n",
            "Batch 1153 Loss: 0.1031\n",
            "Batch 1154 Loss: 0.5106\n",
            "Batch 1155 Loss: 0.2543\n",
            "Batch 1156 Loss: 0.0838\n",
            "Batch 1157 Loss: 0.3104\n",
            "Batch 1158 Loss: 0.0515\n",
            "Batch 1159 Loss: 0.1121\n",
            "Batch 1160 Loss: 0.3818\n",
            "Batch 1161 Loss: 0.1691\n",
            "Batch 1162 Loss: 0.2856\n",
            "Batch 1163 Loss: 0.0380\n",
            "Batch 1164 Loss: 0.0658\n",
            "Batch 1165 Loss: 0.2427\n",
            "Batch 1166 Loss: 0.0188\n",
            "Batch 1167 Loss: 0.0459\n",
            "Batch 1168 Loss: 0.2749\n",
            "Batch 1169 Loss: 0.0700\n",
            "Batch 1170 Loss: 0.1133\n",
            "Batch 1171 Loss: 0.0724\n",
            "Batch 1172 Loss: 0.1656\n",
            "Batch 1173 Loss: 0.1927\n",
            "Batch 1174 Loss: 0.1828\n",
            "Batch 1175 Loss: 0.0474\n",
            "Batch 1176 Loss: 0.1056\n",
            "Batch 1177 Loss: 0.1324\n",
            "Batch 1178 Loss: 0.2740\n",
            "Batch 1179 Loss: 0.0430\n",
            "Batch 1180 Loss: 0.2339\n",
            "Batch 1181 Loss: 0.1179\n",
            "Batch 1182 Loss: 0.0744\n",
            "Batch 1183 Loss: 0.1337\n",
            "Batch 1184 Loss: 0.0762\n",
            "Batch 1185 Loss: 0.0701\n",
            "Batch 1186 Loss: 0.1553\n",
            "Batch 1187 Loss: 0.0291\n",
            "Batch 1188 Loss: 0.1536\n",
            "Batch 1189 Loss: 0.0369\n",
            "Batch 1190 Loss: 0.1857\n",
            "Batch 1191 Loss: 0.0683\n",
            "Batch 1192 Loss: 0.2474\n",
            "Batch 1193 Loss: 0.0222\n",
            "Batch 1194 Loss: 0.0409\n",
            "Batch 1195 Loss: 0.0813\n",
            "Batch 1196 Loss: 0.1519\n",
            "Batch 1197 Loss: 0.0846\n",
            "Batch 1198 Loss: 0.0752\n",
            "Batch 1199 Loss: 0.1779\n",
            "Batch 1200 Loss: 0.1222\n",
            "Batch 1201 Loss: 0.1078\n",
            "Batch 1202 Loss: 0.0325\n",
            "Batch 1203 Loss: 0.1119\n",
            "Batch 1204 Loss: 0.2093\n",
            "Batch 1205 Loss: 0.1793\n",
            "Batch 1206 Loss: 0.3791\n",
            "Batch 1207 Loss: 0.1269\n",
            "Batch 1208 Loss: 0.1845\n",
            "Batch 1209 Loss: 0.2257\n",
            "Batch 1210 Loss: 0.0319\n",
            "Batch 1211 Loss: 0.2174\n",
            "Batch 1212 Loss: 0.0254\n",
            "Batch 1213 Loss: 0.0467\n",
            "Batch 1214 Loss: 0.0453\n",
            "Batch 1215 Loss: 0.4396\n",
            "Batch 1216 Loss: 0.2539\n",
            "Batch 1217 Loss: 0.1021\n",
            "Batch 1218 Loss: 0.0263\n",
            "Batch 1219 Loss: 0.3298\n",
            "Batch 1220 Loss: 0.0811\n",
            "Batch 1221 Loss: 0.1496\n",
            "Batch 1222 Loss: 0.2371\n",
            "Batch 1223 Loss: 0.1128\n",
            "Batch 1224 Loss: 0.1887\n",
            "Batch 1225 Loss: 0.0824\n",
            "Batch 1226 Loss: 0.0771\n",
            "Batch 1227 Loss: 0.0504\n",
            "Batch 1228 Loss: 0.0184\n",
            "Batch 1229 Loss: 0.2483\n",
            "Batch 1230 Loss: 0.0327\n",
            "Batch 1231 Loss: 0.0774\n",
            "Batch 1232 Loss: 0.3213\n",
            "Batch 1233 Loss: 0.0230\n",
            "Batch 1234 Loss: 0.0683\n",
            "Batch 1235 Loss: 0.1098\n",
            "Batch 1236 Loss: 0.1477\n",
            "Batch 1237 Loss: 0.1923\n",
            "Batch 1238 Loss: 0.1472\n",
            "Batch 1239 Loss: 0.0450\n",
            "Batch 1240 Loss: 0.2539\n",
            "Batch 1241 Loss: 0.0384\n",
            "Batch 1242 Loss: 0.6194\n",
            "Batch 1243 Loss: 0.3945\n",
            "Batch 1244 Loss: 0.1037\n",
            "Batch 1245 Loss: 0.0747\n",
            "Batch 1246 Loss: 0.1012\n",
            "Batch 1247 Loss: 0.1349\n",
            "Batch 1248 Loss: 0.0856\n",
            "Batch 1249 Loss: 0.1164\n",
            "Batch 1250 Loss: 0.1167\n",
            "Batch 1251 Loss: 0.0740\n",
            "Batch 1252 Loss: 0.1508\n",
            "Batch 1253 Loss: 0.0246\n",
            "Batch 1254 Loss: 0.1482\n",
            "Batch 1255 Loss: 0.0471\n",
            "Batch 1256 Loss: 0.1526\n",
            "Batch 1257 Loss: 0.0390\n",
            "Batch 1258 Loss: 0.0850\n",
            "Batch 1259 Loss: 0.1143\n",
            "Batch 1260 Loss: 0.2715\n",
            "Batch 1261 Loss: 0.0557\n",
            "Batch 1262 Loss: 0.1680\n",
            "Batch 1263 Loss: 0.4587\n",
            "Batch 1264 Loss: 0.0139\n",
            "Batch 1265 Loss: 0.0268\n",
            "Batch 1266 Loss: 0.1379\n",
            "Batch 1267 Loss: 0.0956\n",
            "Batch 1268 Loss: 0.0795\n",
            "Batch 1269 Loss: 0.2998\n",
            "Batch 1270 Loss: 0.0095\n",
            "Batch 1271 Loss: 0.4358\n",
            "Batch 1272 Loss: 0.0883\n",
            "Batch 1273 Loss: 0.1793\n",
            "Batch 1274 Loss: 0.0855\n",
            "Batch 1275 Loss: 0.0388\n",
            "Batch 1276 Loss: 0.0361\n",
            "Batch 1277 Loss: 0.0881\n",
            "Batch 1278 Loss: 0.2038\n",
            "Batch 1279 Loss: 0.0234\n",
            "Batch 1280 Loss: 0.1297\n",
            "Batch 1281 Loss: 0.3848\n",
            "Batch 1282 Loss: 0.2499\n",
            "Batch 1283 Loss: 0.1406\n",
            "Batch 1284 Loss: 0.2154\n",
            "Batch 1285 Loss: 0.2012\n",
            "Batch 1286 Loss: 0.0766\n",
            "Batch 1287 Loss: 0.0593\n",
            "Batch 1288 Loss: 0.0982\n",
            "Batch 1289 Loss: 0.0389\n",
            "Batch 1290 Loss: 0.0205\n",
            "Batch 1291 Loss: 0.0633\n",
            "Batch 1292 Loss: 0.1947\n",
            "Batch 1293 Loss: 0.0690\n",
            "Batch 1294 Loss: 0.0224\n",
            "Batch 1295 Loss: 0.0409\n",
            "Batch 1296 Loss: 0.1379\n",
            "Batch 1297 Loss: 0.1769\n",
            "Batch 1298 Loss: 0.0242\n",
            "Batch 1299 Loss: 0.0612\n",
            "Batch 1300 Loss: 0.2614\n",
            "Batch 1301 Loss: 0.2246\n",
            "Batch 1302 Loss: 0.0488\n",
            "Batch 1303 Loss: 0.4171\n",
            "Batch 1304 Loss: 0.4249\n",
            "Batch 1305 Loss: 0.3638\n",
            "Batch 1306 Loss: 0.0776\n",
            "Batch 1307 Loss: 0.4507\n",
            "Batch 1308 Loss: 0.1587\n",
            "Batch 1309 Loss: 0.0181\n",
            "Batch 1310 Loss: 0.2882\n",
            "Batch 1311 Loss: 0.1853\n",
            "Batch 1312 Loss: 0.0830\n",
            "Batch 1313 Loss: 0.1140\n",
            "Batch 1314 Loss: 0.1900\n",
            "Batch 1315 Loss: 0.1255\n",
            "Batch 1316 Loss: 0.1402\n",
            "Batch 1317 Loss: 0.0410\n",
            "Batch 1318 Loss: 0.1412\n",
            "Batch 1319 Loss: 0.1597\n",
            "Batch 1320 Loss: 0.1278\n",
            "Batch 1321 Loss: 0.0313\n",
            "Batch 1322 Loss: 0.1972\n",
            "Batch 1323 Loss: 0.0202\n",
            "Batch 1324 Loss: 0.1855\n",
            "Batch 1325 Loss: 0.0830\n",
            "Batch 1326 Loss: 0.2304\n",
            "Batch 1327 Loss: 0.1957\n",
            "Batch 1328 Loss: 0.4485\n",
            "Batch 1329 Loss: 0.1144\n",
            "Batch 1330 Loss: 0.0666\n",
            "Batch 1331 Loss: 0.1272\n",
            "Batch 1332 Loss: 0.1143\n",
            "Batch 1333 Loss: 0.2264\n",
            "Batch 1334 Loss: 0.0282\n",
            "Batch 1335 Loss: 0.0303\n",
            "Batch 1336 Loss: 0.2979\n",
            "Batch 1337 Loss: 0.0788\n",
            "Batch 1338 Loss: 0.0596\n",
            "Batch 1339 Loss: 0.1136\n",
            "Batch 1340 Loss: 0.0780\n",
            "Batch 1341 Loss: 0.1379\n",
            "Batch 1342 Loss: 0.0704\n",
            "Batch 1343 Loss: 0.1661\n",
            "Batch 1344 Loss: 0.2233\n",
            "Batch 1345 Loss: 0.2273\n",
            "Batch 1346 Loss: 0.0840\n",
            "Batch 1347 Loss: 0.1351\n",
            "Batch 1348 Loss: 0.2385\n",
            "Batch 1349 Loss: 0.1288\n",
            "Batch 1350 Loss: 0.0182\n",
            "Batch 1351 Loss: 0.0452\n",
            "Batch 1352 Loss: 0.2577\n",
            "Batch 1353 Loss: 0.0407\n",
            "Batch 1354 Loss: 0.0882\n",
            "Batch 1355 Loss: 0.0991\n",
            "Batch 1356 Loss: 0.0378\n",
            "Batch 1357 Loss: 0.1603\n",
            "Batch 1358 Loss: 0.0794\n",
            "Batch 1359 Loss: 0.1225\n",
            "Batch 1360 Loss: 0.0989\n",
            "Batch 1361 Loss: 0.4195\n",
            "Batch 1362 Loss: 0.0507\n",
            "Batch 1363 Loss: 0.0631\n",
            "Batch 1364 Loss: 0.0646\n",
            "Batch 1365 Loss: 0.0212\n",
            "Batch 1366 Loss: 0.0632\n",
            "Batch 1367 Loss: 0.1048\n",
            "Batch 1368 Loss: 0.0281\n",
            "Batch 1369 Loss: 0.2537\n",
            "Batch 1370 Loss: 0.0763\n",
            "Batch 1371 Loss: 0.0451\n",
            "Batch 1372 Loss: 0.1394\n",
            "Batch 1373 Loss: 0.0916\n",
            "Batch 1374 Loss: 0.3135\n",
            "Batch 1375 Loss: 0.1662\n",
            "Batch 1376 Loss: 0.0754\n",
            "Batch 1377 Loss: 0.2380\n",
            "Batch 1378 Loss: 0.0052\n",
            "Batch 1379 Loss: 0.0240\n",
            "Batch 1380 Loss: 0.2103\n",
            "Batch 1381 Loss: 0.0150\n",
            "Batch 1382 Loss: 0.0714\n",
            "Batch 1383 Loss: 0.0465\n",
            "Batch 1384 Loss: 0.2192\n",
            "Batch 1385 Loss: 0.0956\n",
            "Batch 1386 Loss: 0.0387\n",
            "Batch 1387 Loss: 0.1755\n",
            "Batch 1388 Loss: 0.2241\n",
            "Batch 1389 Loss: 0.1123\n",
            "Batch 1390 Loss: 0.1416\n",
            "Batch 1391 Loss: 0.2221\n",
            "Batch 1392 Loss: 0.0180\n",
            "Batch 1393 Loss: 0.0331\n",
            "Batch 1394 Loss: 0.0420\n",
            "Batch 1395 Loss: 0.0729\n",
            "Batch 1396 Loss: 0.2780\n",
            "Batch 1397 Loss: 0.0640\n",
            "Batch 1398 Loss: 0.0943\n",
            "Batch 1399 Loss: 0.2233\n",
            "Batch 1400 Loss: 0.0477\n",
            "Batch 1401 Loss: 0.1819\n",
            "Batch 1402 Loss: 0.0388\n",
            "Batch 1403 Loss: 0.1739\n",
            "Batch 1404 Loss: 0.0477\n",
            "Batch 1405 Loss: 0.1139\n",
            "Batch 1406 Loss: 0.0405\n",
            "Batch 1407 Loss: 0.1230\n",
            "Batch 1408 Loss: 0.1001\n",
            "Batch 1409 Loss: 0.3737\n",
            "Batch 1410 Loss: 0.1383\n",
            "Batch 1411 Loss: 0.3743\n",
            "Batch 1412 Loss: 0.1526\n",
            "Batch 1413 Loss: 0.1341\n",
            "Batch 1414 Loss: 0.1476\n",
            "Batch 1415 Loss: 0.0647\n",
            "Batch 1416 Loss: 0.0133\n",
            "Batch 1417 Loss: 0.1069\n",
            "Batch 1418 Loss: 0.3987\n",
            "Batch 1419 Loss: 0.2452\n",
            "Batch 1420 Loss: 0.0693\n",
            "Batch 1421 Loss: 0.2315\n",
            "Batch 1422 Loss: 0.0375\n",
            "Batch 1423 Loss: 0.1422\n",
            "Batch 1424 Loss: 0.0516\n",
            "Batch 1425 Loss: 0.2009\n",
            "Batch 1426 Loss: 0.0533\n",
            "Batch 1427 Loss: 0.2864\n",
            "Batch 1428 Loss: 0.0895\n",
            "Batch 1429 Loss: 0.1219\n",
            "Batch 1430 Loss: 0.1567\n",
            "Batch 1431 Loss: 0.0227\n",
            "Batch 1432 Loss: 0.1332\n",
            "Batch 1433 Loss: 0.1159\n",
            "Batch 1434 Loss: 0.2576\n",
            "Batch 1435 Loss: 0.1931\n",
            "Batch 1436 Loss: 0.1670\n",
            "Batch 1437 Loss: 0.2081\n",
            "Batch 1438 Loss: 0.1657\n",
            "Batch 1439 Loss: 0.0659\n",
            "Batch 1440 Loss: 0.1997\n",
            "Batch 1441 Loss: 0.0717\n",
            "Batch 1442 Loss: 0.2247\n",
            "Batch 1443 Loss: 0.0560\n",
            "Batch 1444 Loss: 0.0750\n",
            "Batch 1445 Loss: 0.2497\n",
            "Batch 1446 Loss: 0.0258\n",
            "Batch 1447 Loss: 0.1334\n",
            "Batch 1448 Loss: 0.2350\n",
            "Batch 1449 Loss: 0.1151\n",
            "Batch 1450 Loss: 0.0105\n",
            "Batch 1451 Loss: 0.0950\n",
            "Batch 1452 Loss: 0.0098\n",
            "Batch 1453 Loss: 0.1482\n",
            "Batch 1454 Loss: 0.2185\n",
            "Batch 1455 Loss: 0.2147\n",
            "Batch 1456 Loss: 0.0508\n",
            "Batch 1457 Loss: 0.1970\n",
            "Batch 1458 Loss: 0.0901\n",
            "Batch 1459 Loss: 0.2179\n",
            "Batch 1460 Loss: 0.0520\n",
            "Batch 1461 Loss: 0.0740\n",
            "Batch 1462 Loss: 0.0267\n",
            "Batch 1463 Loss: 0.0312\n",
            "Batch 1464 Loss: 0.0136\n",
            "Batch 1465 Loss: 0.0390\n",
            "Batch 1466 Loss: 0.1183\n",
            "Batch 1467 Loss: 0.2461\n",
            "Batch 1468 Loss: 0.1971\n",
            "Batch 1469 Loss: 0.0346\n",
            "Batch 1470 Loss: 0.0821\n",
            "Batch 1471 Loss: 0.0612\n",
            "Batch 1472 Loss: 0.0294\n",
            "Batch 1473 Loss: 0.0269\n",
            "Batch 1474 Loss: 0.0165\n",
            "Batch 1475 Loss: 0.1814\n",
            "Batch 1476 Loss: 0.0948\n",
            "Batch 1477 Loss: 0.0452\n",
            "Batch 1478 Loss: 0.3160\n",
            "Batch 1479 Loss: 0.1868\n",
            "Batch 1480 Loss: 0.0966\n",
            "Batch 1481 Loss: 0.0885\n",
            "Batch 1482 Loss: 0.0167\n",
            "Batch 1483 Loss: 0.0612\n",
            "Batch 1484 Loss: 0.0412\n",
            "Batch 1485 Loss: 0.1761\n",
            "Batch 1486 Loss: 0.0578\n",
            "Batch 1487 Loss: 0.0458\n",
            "Batch 1488 Loss: 0.1109\n",
            "Batch 1489 Loss: 0.2149\n",
            "Batch 1490 Loss: 0.1834\n",
            "Batch 1491 Loss: 0.0625\n",
            "Batch 1492 Loss: 0.0248\n",
            "Batch 1493 Loss: 0.0580\n",
            "Batch 1494 Loss: 0.0655\n",
            "Batch 1495 Loss: 0.1565\n",
            "Batch 1496 Loss: 0.1843\n",
            "Batch 1497 Loss: 0.3041\n",
            "Batch 1498 Loss: 0.1793\n",
            "Batch 1499 Loss: 0.1322\n",
            "Batch 1500 Loss: 0.0896\n",
            "Batch 1501 Loss: 0.0614\n",
            "Batch 1502 Loss: 0.1949\n",
            "Batch 1503 Loss: 0.1457\n",
            "Batch 1504 Loss: 0.1583\n",
            "Batch 1505 Loss: 0.0232\n",
            "Batch 1506 Loss: 0.2050\n",
            "Batch 1507 Loss: 0.0327\n",
            "Batch 1508 Loss: 0.1039\n",
            "Batch 1509 Loss: 0.1300\n",
            "Batch 1510 Loss: 0.1551\n",
            "Batch 1511 Loss: 0.0329\n",
            "Batch 1512 Loss: 0.1142\n",
            "Batch 1513 Loss: 0.0512\n",
            "Batch 1514 Loss: 0.2228\n",
            "Batch 1515 Loss: 0.0787\n",
            "Batch 1516 Loss: 0.1630\n",
            "Batch 1517 Loss: 0.2345\n",
            "Batch 1518 Loss: 0.1483\n",
            "Batch 1519 Loss: 0.1227\n",
            "Batch 1520 Loss: 0.0546\n",
            "Batch 1521 Loss: 0.0126\n",
            "Batch 1522 Loss: 0.1467\n",
            "Batch 1523 Loss: 0.0641\n",
            "Batch 1524 Loss: 0.1096\n",
            "Batch 1525 Loss: 0.3339\n",
            "Batch 1526 Loss: 0.2423\n",
            "Batch 1527 Loss: 0.0119\n",
            "Batch 1528 Loss: 0.0534\n",
            "Batch 1529 Loss: 0.0130\n",
            "Batch 1530 Loss: 0.0732\n",
            "Batch 1531 Loss: 0.0148\n",
            "Batch 1532 Loss: 0.0621\n",
            "Batch 1533 Loss: 0.1584\n",
            "Batch 1534 Loss: 0.0308\n",
            "Batch 1535 Loss: 0.2583\n",
            "Batch 1536 Loss: 0.1561\n",
            "Batch 1537 Loss: 0.3967\n",
            "Batch 1538 Loss: 0.0456\n",
            "Batch 1539 Loss: 0.2430\n",
            "Batch 1540 Loss: 0.0753\n",
            "Batch 1541 Loss: 0.1591\n",
            "Batch 1542 Loss: 0.1821\n",
            "Batch 1543 Loss: 0.0670\n",
            "Batch 1544 Loss: 0.0223\n",
            "Batch 1545 Loss: 0.2574\n",
            "Batch 1546 Loss: 0.1038\n",
            "Batch 1547 Loss: 0.0424\n",
            "Batch 1548 Loss: 0.2070\n",
            "Batch 1549 Loss: 0.0464\n",
            "Batch 1550 Loss: 0.0778\n",
            "Batch 1551 Loss: 0.1425\n",
            "Batch 1552 Loss: 0.2842\n",
            "Batch 1553 Loss: 0.0697\n",
            "Batch 1554 Loss: 0.0885\n",
            "Batch 1555 Loss: 0.0276\n",
            "Batch 1556 Loss: 0.2054\n",
            "Batch 1557 Loss: 0.3852\n",
            "Batch 1558 Loss: 0.0556\n",
            "Batch 1559 Loss: 0.1846\n",
            "Batch 1560 Loss: 0.0436\n",
            "Batch 1561 Loss: 0.1859\n",
            "Batch 1562 Loss: 0.0864\n",
            "Batch 1563 Loss: 0.2426\n",
            "Batch 1564 Loss: 0.0378\n",
            "Batch 1565 Loss: 0.5074\n",
            "Batch 1566 Loss: 0.2547\n",
            "Batch 1567 Loss: 0.2322\n",
            "Batch 1568 Loss: 0.1933\n",
            "Batch 1569 Loss: 0.1375\n",
            "Batch 1570 Loss: 0.0197\n",
            "Batch 1571 Loss: 0.0425\n",
            "Batch 1572 Loss: 0.1687\n",
            "Batch 1573 Loss: 0.0571\n",
            "Batch 1574 Loss: 0.0472\n",
            "Batch 1575 Loss: 0.0727\n",
            "Batch 1576 Loss: 0.0558\n",
            "Batch 1577 Loss: 0.0300\n",
            "Batch 1578 Loss: 0.0707\n",
            "Batch 1579 Loss: 0.2849\n",
            "Batch 1580 Loss: 0.3163\n",
            "Batch 1581 Loss: 0.0834\n",
            "Batch 1582 Loss: 0.0395\n",
            "Batch 1583 Loss: 0.1927\n",
            "Batch 1584 Loss: 0.0476\n",
            "Batch 1585 Loss: 0.1846\n",
            "Batch 1586 Loss: 0.0846\n",
            "Batch 1587 Loss: 0.0767\n",
            "Batch 1588 Loss: 0.2071\n",
            "Batch 1589 Loss: 0.0258\n",
            "Batch 1590 Loss: 0.0231\n",
            "Batch 1591 Loss: 0.2676\n",
            "Batch 1592 Loss: 0.0208\n",
            "Batch 1593 Loss: 0.1054\n",
            "Batch 1594 Loss: 0.1883\n",
            "Batch 1595 Loss: 0.1207\n",
            "Batch 1596 Loss: 0.0565\n",
            "Batch 1597 Loss: 0.1551\n",
            "Batch 1598 Loss: 0.0220\n",
            "Batch 1599 Loss: 0.0366\n",
            "Batch 1600 Loss: 0.0500\n",
            "Batch 1601 Loss: 0.1473\n",
            "Batch 1602 Loss: 0.2020\n",
            "Batch 1603 Loss: 0.1414\n",
            "Batch 1604 Loss: 0.1496\n",
            "Batch 1605 Loss: 0.0388\n",
            "Batch 1606 Loss: 0.3431\n",
            "Batch 1607 Loss: 0.0426\n",
            "Batch 1608 Loss: 0.0773\n",
            "Batch 1609 Loss: 0.2359\n",
            "Batch 1610 Loss: 0.0163\n",
            "Batch 1611 Loss: 0.0585\n",
            "Batch 1612 Loss: 0.0775\n",
            "Batch 1613 Loss: 0.0287\n",
            "Batch 1614 Loss: 0.1036\n",
            "Batch 1615 Loss: 0.0142\n",
            "Batch 1616 Loss: 0.1052\n",
            "Batch 1617 Loss: 0.1888\n",
            "Batch 1618 Loss: 0.1746\n",
            "Batch 1619 Loss: 0.1731\n",
            "Batch 1620 Loss: 0.0124\n",
            "Batch 1621 Loss: 0.1282\n",
            "Batch 1622 Loss: 0.2162\n",
            "Batch 1623 Loss: 0.0393\n",
            "Batch 1624 Loss: 0.0426\n",
            "Batch 1625 Loss: 0.1030\n",
            "Batch 1626 Loss: 0.1893\n",
            "Batch 1627 Loss: 0.2351\n",
            "Batch 1628 Loss: 0.1453\n",
            "Batch 1629 Loss: 0.0927\n",
            "Batch 1630 Loss: 0.0608\n",
            "Batch 1631 Loss: 0.1875\n",
            "Batch 1632 Loss: 0.0281\n",
            "Batch 1633 Loss: 0.2262\n",
            "Batch 1634 Loss: 0.0343\n",
            "Batch 1635 Loss: 0.0259\n",
            "Batch 1636 Loss: 0.1663\n",
            "Batch 1637 Loss: 0.2058\n",
            "Batch 1638 Loss: 0.0656\n",
            "Batch 1639 Loss: 0.2053\n",
            "Batch 1640 Loss: 0.1819\n",
            "Batch 1641 Loss: 0.0761\n",
            "Batch 1642 Loss: 0.0736\n",
            "Batch 1643 Loss: 0.1998\n",
            "Batch 1644 Loss: 0.0975\n",
            "Batch 1645 Loss: 0.1754\n",
            "Batch 1646 Loss: 0.1025\n",
            "Batch 1647 Loss: 0.0357\n",
            "Batch 1648 Loss: 0.1729\n",
            "Batch 1649 Loss: 0.3441\n",
            "Batch 1650 Loss: 0.1014\n",
            "Batch 1651 Loss: 0.1235\n",
            "Batch 1652 Loss: 0.0178\n",
            "Batch 1653 Loss: 0.2580\n",
            "Batch 1654 Loss: 0.1545\n",
            "Batch 1655 Loss: 0.0592\n",
            "Batch 1656 Loss: 0.0954\n",
            "Batch 1657 Loss: 0.1925\n",
            "Batch 1658 Loss: 0.0764\n",
            "Batch 1659 Loss: 0.2558\n",
            "Batch 1660 Loss: 0.2911\n",
            "Batch 1661 Loss: 0.1152\n",
            "Batch 1662 Loss: 0.0444\n",
            "Batch 1663 Loss: 0.0974\n",
            "Batch 1664 Loss: 0.2698\n",
            "Batch 1665 Loss: 0.1176\n",
            "Batch 1666 Loss: 0.1570\n",
            "Batch 1667 Loss: 0.2815\n",
            "Batch 1668 Loss: 0.4418\n",
            "Batch 1669 Loss: 0.0712\n",
            "Batch 1670 Loss: 0.0588\n",
            "Batch 1671 Loss: 0.2254\n",
            "Batch 1672 Loss: 0.0972\n",
            "Batch 1673 Loss: 0.0568\n",
            "Batch 1674 Loss: 0.2540\n",
            "Batch 1675 Loss: 0.0280\n",
            "Batch 1676 Loss: 0.1415\n",
            "Batch 1677 Loss: 0.0143\n",
            "Batch 1678 Loss: 0.1194\n",
            "Batch 1679 Loss: 0.1427\n",
            "Batch 1680 Loss: 0.0208\n",
            "Batch 1681 Loss: 0.0811\n",
            "Batch 1682 Loss: 0.0623\n",
            "Batch 1683 Loss: 0.0221\n",
            "Batch 1684 Loss: 0.4534\n",
            "Batch 1685 Loss: 0.1062\n",
            "Batch 1686 Loss: 0.0834\n",
            "Batch 1687 Loss: 0.0540\n",
            "Batch 1688 Loss: 0.0106\n",
            "Batch 1689 Loss: 0.0207\n",
            "Batch 1690 Loss: 0.0175\n",
            "Batch 1691 Loss: 0.0288\n",
            "Batch 1692 Loss: 0.0431\n",
            "Batch 1693 Loss: 0.2306\n",
            "Batch 1694 Loss: 0.0066\n",
            "Batch 1695 Loss: 0.0882\n",
            "Batch 1696 Loss: 0.0257\n",
            "Batch 1697 Loss: 0.0304\n",
            "Batch 1698 Loss: 0.1736\n",
            "Batch 1699 Loss: 0.1207\n",
            "Batch 1700 Loss: 0.1073\n",
            "Batch 1701 Loss: 0.1327\n",
            "Batch 1702 Loss: 0.1780\n",
            "Batch 1703 Loss: 0.0694\n",
            "Batch 1704 Loss: 0.1166\n",
            "Batch 1705 Loss: 0.0835\n",
            "Batch 1706 Loss: 0.0484\n",
            "Batch 1707 Loss: 0.1539\n",
            "Batch 1708 Loss: 0.1633\n",
            "Batch 1709 Loss: 0.0594\n",
            "Batch 1710 Loss: 0.3381\n",
            "Batch 1711 Loss: 0.0889\n",
            "Batch 1712 Loss: 0.0304\n",
            "Batch 1713 Loss: 0.0737\n",
            "Batch 1714 Loss: 0.3794\n",
            "Batch 1715 Loss: 0.1960\n",
            "Batch 1716 Loss: 0.0441\n",
            "Batch 1717 Loss: 0.2320\n",
            "Batch 1718 Loss: 0.1866\n",
            "Batch 1719 Loss: 0.0194\n",
            "Batch 1720 Loss: 0.0867\n",
            "Batch 1721 Loss: 0.0698\n",
            "Batch 1722 Loss: 0.2462\n",
            "Batch 1723 Loss: 0.0282\n",
            "Batch 1724 Loss: 0.0163\n",
            "Batch 1725 Loss: 0.0258\n",
            "Batch 1726 Loss: 0.1273\n",
            "Batch 1727 Loss: 0.1552\n",
            "Batch 1728 Loss: 0.1695\n",
            "Batch 1729 Loss: 0.1507\n",
            "Batch 1730 Loss: 0.1276\n",
            "Batch 1731 Loss: 0.0186\n",
            "Batch 1732 Loss: 0.0644\n",
            "Batch 1733 Loss: 0.1349\n",
            "Batch 1734 Loss: 0.2507\n",
            "Batch 1735 Loss: 0.1206\n",
            "Batch 1736 Loss: 0.0152\n",
            "Batch 1737 Loss: 0.0722\n",
            "Batch 1738 Loss: 0.1258\n",
            "Batch 1739 Loss: 0.3582\n",
            "Batch 1740 Loss: 0.0354\n",
            "Batch 1741 Loss: 0.0112\n",
            "Batch 1742 Loss: 0.0347\n",
            "Batch 1743 Loss: 0.0475\n",
            "Batch 1744 Loss: 0.2370\n",
            "Batch 1745 Loss: 0.0488\n",
            "Batch 1746 Loss: 0.0877\n",
            "Batch 1747 Loss: 0.0903\n",
            "Batch 1748 Loss: 0.0637\n",
            "Batch 1749 Loss: 0.2439\n",
            "Batch 1750 Loss: 0.0375\n",
            "Batch 1751 Loss: 0.1283\n",
            "Batch 1752 Loss: 0.1044\n",
            "Batch 1753 Loss: 0.0475\n",
            "Batch 1754 Loss: 0.0568\n",
            "Batch 1755 Loss: 0.0849\n",
            "Batch 1756 Loss: 0.1332\n",
            "Batch 1757 Loss: 0.1757\n",
            "Batch 1758 Loss: 0.1782\n",
            "Batch 1759 Loss: 0.1827\n",
            "Batch 1760 Loss: 0.1024\n",
            "Batch 1761 Loss: 0.0292\n",
            "Batch 1762 Loss: 0.0479\n",
            "Batch 1763 Loss: 0.0134\n",
            "Batch 1764 Loss: 0.0740\n",
            "Batch 1765 Loss: 0.0848\n",
            "Batch 1766 Loss: 0.0764\n",
            "Batch 1767 Loss: 0.2213\n",
            "Batch 1768 Loss: 0.0900\n",
            "Batch 1769 Loss: 0.0271\n",
            "Batch 1770 Loss: 0.1822\n",
            "Batch 1771 Loss: 0.0587\n",
            "Batch 1772 Loss: 0.0074\n",
            "Batch 1773 Loss: 0.0719\n",
            "Batch 1774 Loss: 0.0574\n",
            "Batch 1775 Loss: 0.0054\n",
            "Batch 1776 Loss: 0.0187\n",
            "Batch 1777 Loss: 0.0444\n",
            "Batch 1778 Loss: 0.0693\n",
            "Batch 1779 Loss: 0.0165\n",
            "Batch 1780 Loss: 0.2419\n",
            "Batch 1781 Loss: 0.1835\n",
            "Batch 1782 Loss: 0.1255\n",
            "Batch 1783 Loss: 0.0359\n",
            "Batch 1784 Loss: 0.0358\n",
            "Batch 1785 Loss: 0.2064\n",
            "Batch 1786 Loss: 0.4668\n",
            "Batch 1787 Loss: 0.0394\n",
            "Batch 1788 Loss: 0.0108\n",
            "Batch 1789 Loss: 0.0706\n",
            "Batch 1790 Loss: 0.2418\n",
            "Batch 1791 Loss: 0.2181\n",
            "Batch 1792 Loss: 0.1014\n",
            "Batch 1793 Loss: 0.1158\n",
            "Batch 1794 Loss: 0.0863\n",
            "Batch 1795 Loss: 0.0031\n",
            "Batch 1796 Loss: 0.2314\n",
            "Batch 1797 Loss: 0.0335\n",
            "Batch 1798 Loss: 0.1782\n",
            "Batch 1799 Loss: 0.0755\n",
            "Batch 1800 Loss: 0.0467\n",
            "Batch 1801 Loss: 0.0364\n",
            "Batch 1802 Loss: 0.1399\n",
            "Batch 1803 Loss: 0.0182\n",
            "Batch 1804 Loss: 0.0220\n",
            "Batch 1805 Loss: 0.0775\n",
            "Batch 1806 Loss: 0.0363\n",
            "Batch 1807 Loss: 0.1182\n",
            "Batch 1808 Loss: 0.1229\n",
            "Batch 1809 Loss: 0.0278\n",
            "Batch 1810 Loss: 0.1933\n",
            "Batch 1811 Loss: 0.0612\n",
            "Batch 1812 Loss: 0.1794\n",
            "Batch 1813 Loss: 0.5597\n",
            "Batch 1814 Loss: 0.0535\n",
            "Batch 1815 Loss: 0.2748\n",
            "Batch 1816 Loss: 0.2775\n",
            "Batch 1817 Loss: 0.0210\n",
            "Batch 1818 Loss: 0.0116\n",
            "Batch 1819 Loss: 0.0098\n",
            "Batch 1820 Loss: 0.2603\n",
            "Batch 1821 Loss: 0.3996\n",
            "Batch 1822 Loss: 0.0679\n",
            "Batch 1823 Loss: 0.1366\n",
            "Batch 1824 Loss: 0.0037\n",
            "Batch 1825 Loss: 0.1984\n",
            "Batch 1826 Loss: 0.1899\n",
            "Batch 1827 Loss: 0.0778\n",
            "Batch 1828 Loss: 0.1354\n",
            "Batch 1829 Loss: 0.1250\n",
            "Batch 1830 Loss: 0.0270\n",
            "Batch 1831 Loss: 0.2759\n",
            "Batch 1832 Loss: 0.0196\n",
            "Batch 1833 Loss: 0.3110\n",
            "Batch 1834 Loss: 0.0994\n",
            "Batch 1835 Loss: 0.0248\n",
            "Batch 1836 Loss: 0.0474\n",
            "Batch 1837 Loss: 0.3645\n",
            "Batch 1838 Loss: 0.0621\n",
            "Batch 1839 Loss: 0.1891\n",
            "Batch 1840 Loss: 0.1411\n",
            "Batch 1841 Loss: 0.0246\n",
            "Batch 1842 Loss: 0.4796\n",
            "Batch 1843 Loss: 0.1760\n",
            "Batch 1844 Loss: 0.0875\n",
            "Batch 1845 Loss: 0.0189\n",
            "Batch 1846 Loss: 0.0810\n",
            "Batch 1847 Loss: 0.0526\n",
            "Batch 1848 Loss: 0.1598\n",
            "Batch 1849 Loss: 0.4026\n",
            "Batch 1850 Loss: 0.1232\n",
            "Batch 1851 Loss: 0.3304\n",
            "Batch 1852 Loss: 0.2973\n",
            "Batch 1853 Loss: 0.0218\n",
            "Batch 1854 Loss: 0.0928\n",
            "Batch 1855 Loss: 0.0161\n",
            "Batch 1856 Loss: 0.1207\n",
            "Batch 1857 Loss: 0.1806\n",
            "Batch 1858 Loss: 0.1017\n",
            "Batch 1859 Loss: 0.0291\n",
            "Batch 1860 Loss: 0.1879\n",
            "Batch 1861 Loss: 0.0164\n",
            "Batch 1862 Loss: 0.0434\n",
            "Batch 1863 Loss: 0.1680\n",
            "Batch 1864 Loss: 0.0447\n",
            "Batch 1865 Loss: 0.0394\n",
            "Batch 1866 Loss: 0.0162\n",
            "Batch 1867 Loss: 0.0498\n",
            "Batch 1868 Loss: 0.2000\n",
            "Batch 1869 Loss: 0.0816\n",
            "Batch 1870 Loss: 0.0837\n",
            "Batch 1871 Loss: 0.2849\n",
            "Batch 1872 Loss: 0.1017\n",
            "Batch 1873 Loss: 0.0943\n",
            "Batch 1874 Loss: 0.1495\n",
            "Batch 1875 Loss: 0.0288\n",
            "Average Loss after Epoch 1: 0.2237\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-294c87411c79>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {accuracy * 100:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-dbaea3f78809>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_images)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions =np.argmax(cnn.forward(test_images),axis =1 )\n",
        "accuracy = np.mean(predictions == test_labels)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ORX8F8Nb72H",
        "outputId": "b86aebed-3f24-4802-cf01-e367d80b9178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3fb70eaab3f5>:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  output[b, f, i, j] = np.sum(region * self.filters[f]) + self.biases[f]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 96.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def visualize_prediction(model, test_images, test_labels, index=0):\n",
        "    image = test_images[index]\n",
        "    true_label = test_labels[index]\n",
        "\n",
        "    predicted_label = np.argmax(model.forward(np.expand_dims(image, axis=0)),axis=1) # Thêm một chiều batch\n",
        "\n",
        "    plt.imshow(image.squeeze(), cmap='gray')  # .squeeze() để loại bỏ chiều dư thừa\n",
        "    plt.title(f\"True Label: {true_label}, Predicted: {predicted_label[0]}\")\n",
        "    plt.axis('off')  # Tắt trục\n",
        "    plt.show()\n",
        "visualize_prediction(cnn,test_images=test_images,test_labels=test_labels,index=240)\n",
        "visualize_prediction(cnn,test_images=test_images,test_labels=test_labels,index=520)\n",
        "visualize_prediction(cnn,test_images=test_images,test_labels=test_labels,index=740)\n",
        "visualize_prediction(cnn,test_images=test_images,test_labels=test_labels,index=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ugaYXTlKe5Rr",
        "outputId": "22eadad9-f614-4ee2-c758-d2e8f1ccd247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3fb70eaab3f5>:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  output[b, f, i, j] = np.sum(region * self.filters[f]) + self.biases[f]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXhUlEQVR4nO3ce3BU5RnH8d9KICEgkhBomNCGEALlVpBYpRZIJChQLsolaaYgBFDkpkgLlNShEkGtgq3TgBamNYgylXuLY6ug0rEjF8eWBgiDDZBYIBQCCI1AxkDe/sHwjMsmJGdDTIDvZ4YZ93Ce3Zdd2G/O7vH4nHNOAABIuq2uFwAAqD+IAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAq6b+fPny+fz6eTJk9ftPjMyMtS2bdvrdn83gxUrVsjn86mwsNC2JScnKzk5uc7WdLWK1ogbA1GoJT6fr1q//va3v9XpOpOTk9W1a9c6XUNtKi0t1fPPP6/OnTsrPDxcMTExSk1NVV5eXtD32bZtW7/XsFWrVurTp482btx4HVde+86fP6/58+fX+d/ByqxZs0a9evVS8+bN1aJFCyUlJemdd96p62Xd9ELqegE3qzfeeMPv9sqVK7Vly5aA7Z06dfoml3XLGT16tDZt2qRHH31UPXv2VFFRkZYuXaof/OAH2rNnj2JjY4O63x49euhnP/uZJKmoqEjLli3TiBEj9Oqrr2ry5MnX849QLZs3b/Y8c/78eWVlZUlSvTrKkKTs7Gw98cQTGjx4sH71q1+ptLRUK1as0JAhQ7R+/XqNGDGirpd40yIKtWTMmDF+t3fs2KEtW7YEbL/a+fPnFR4eXptLu2UcPXpUGzZs0KxZs7Ro0SLb3qdPH/Xr108bNmzQzJkzg7rvmJgYv9dy7Nixat++vX7zm99UGoWLFy+qvLxcjRo1Cuoxr6U27rMuZWdn6/vf/77efvtt+Xw+SdKECRMUExOj119/nSjUIj4+qkNXPrr5xz/+ob59+yo8PFy/+MUvJF3++Gn+/PkBM23btlVGRobftjNnzujJJ5/Ut7/9bYWGhqp9+/Z64YUXVF5efl3WuXv3bmVkZKhdu3YKCwtTdHS0JkyYoFOnTlW4/8mTJ5WWlqZmzZqpRYsWmjFjhkpLSwP2e/PNN5WYmKjGjRsrMjJS6enpOnz4cJXrOXbsmPbv36+ysrJr7ldSUiJJ+ta3vuW3vXXr1pKkxo0bV/lY1RUdHa1OnTqpoKBAklRYWCifz6fFixfr5ZdfVnx8vEJDQ7Vv3z5J0v79+zVq1ChFRkYqLCxMd911lzZt2hRwv3l5eerXr58aN26sNm3aaOHChRW+rhV9p1BaWqr58+erQ4cOCgsLU+vWrTVixAgdPHhQhYWFatmypSQpKyvLPgr7+t+5673Gs2fPav/+/Tp79myVz+f//vc/tWrVyoIgSc2aNVPTpk2v6+uGQBwp1LFTp05p0KBBSk9P15gxYwLewKpy/vx5JSUl6ejRo3rsscf0ne98R9u2bVNmZqaOHTuml19+ucZr3LJliw4dOqTx48crOjpaeXl5Wr58ufLy8rRjxw6/f7iSlJaWprZt2+r555/Xjh079Nvf/lZffPGFVq5cafs8++yzmjdvntLS0vTII4+ouLhY2dnZ6tu3r3bt2qXmzZtXup7MzEy9/vrrKigouOaX0PHx8WrTpo1eeukldezYUXfeeaeKioo0Z84cxcXFKT09vaZPjSkrK9Phw4fVokULv+05OTkqLS3VpEmTFBoaqsjISOXl5emHP/yhYmJiNHfuXDVp0kRr1qzRQw89pPXr12v48OGSpP/+97+67777dPHiRdtv+fLl1XpTvHTpkoYMGaIPPvhA6enpmjFjhkpKSrRlyxbt3btX/fv316uvvqopU6Zo+PDh9pP39773PUmqlTVu3LhR48ePV05OTsAPNldLTk7WunXrlJ2draFDh6q0tFTZ2dk6e/asZsyYUeWfHzXg8I2YNm2au/rpTkpKcpLc7373u4D9Jbmnn346YHtsbKwbN26c3V6wYIFr0qSJ+/e//+2339y5c12DBg3cf/7zn2uuKykpyXXp0uWa+5w/fz5g2x//+EcnyX300Ue27emnn3aS3LBhw/z2nTp1qpPkcnNznXPOFRYWugYNGrhnn33Wb789e/a4kJAQv+3jxo1zsbGxfvuNGzfOSXIFBQXXXLdzzu3cudPFx8c7SfYrMTHRHTt2rMrZysTGxroHHnjAFRcXu+LiYpebm+vS09OdJPf4448755wrKChwklyzZs3ciRMn/OZTUlJct27dXGlpqW0rLy939957r0tISLBtTz75pJPkdu7cadtOnDjh7rjjjoA/f1JSkktKSrLbr732mpPkfv3rXwesv7y83DnnXHFxcaV/z2pjjTk5OU6Sy8nJCXi8qx0/ftylpKT4vW5RUVFu27ZtVc6iZvj4qI6FhoZq/PjxQc+vXbtWffr0UUREhE6ePGm/+vfvr0uXLumjjz6q8Rq//lNfaWmpTp48qV69ekmS/vnPfwbsP23aNL/bjz/+uCTpL3/5iyRpw4YNKi8vV1pamt+ao6OjlZCQoK1bt15zPStWrJBzrlqnqkZERKhHjx6aO3eu/vSnP2nx4sUqLCxUampqhR9pVdfmzZvVsmVLtWzZUt27d9fatWv18MMP64UXXvDbb+TIkfYxjSSdPn1aH374odLS0lRSUmJ/9lOnTmnAgAHKz8/X0aNHJV1+vnr16qW7777b5lu2bKnRo0dXub7169crKirKnvuvu/rI7mq1tcaMjAw556o8SpCk8PBwdezYUePGjdPatWv12muv2cdfBw4cqHIewePjozoWExNToy8J8/PztXv3br83nq87ceJE0Pd9xenTp5WVlaW33nor4P4q+nw4ISHB73Z8fLxuu+02O2c9Pz9fzrmA/a5o2LBhjdd8ZW19+vTR7Nmz7UwhSbrrrruUnJysnJwcTZkyJaj7vueee7Rw4UL5fD6Fh4erU6dOFX7kFRcX53f7wIEDcs5p3rx5mjdvXoX3feLECcXExOjzzz/XPffcE/D7HTt2rHJ9Bw8eVMeOHRUS4v2f+De1xmtJTU1VSEiI3n77bdv24IMPKiEhQU899ZRWr15do/tH5YhCHfP6pdmlS5f8bpeXl+v+++/XnDlzKty/Q4cOQa/tirS0NG3btk2zZ89Wjx491LRpU5WXl2vgwIHV+jL76p9My8vL5fP59Ne//lUNGjQI2L9p06Y1XrN0+afl48ePa9iwYX7bk5KS1KxZM3388cdBRyEqKkr9+/evcr+rX98rz9esWbM0YMCACmfat28f1Jqul7pe46FDh/Tuu+9q+fLlftsjIyPVu3dvffzxx7X22CAK9VZERITOnDnjt+2rr77SsWPH/LbFx8fryy+/rNYbVDC++OILffDBB8rKytIvf/lL256fn1/pTH5+vt9PyAcOHFB5ebl93BMfHy/nnOLi4q5LtCpz/PhxSYEhdc7p0qVLunjxYq09dmXatWsn6fLRUFWvWWxsbIXP82effVbl48THx2vnzp0qKyur9Mirso+Rvqk1Vqay1026/IV+XbxutxK+U6in4uPjA74PWL58ecA/lLS0NG3fvl3vvfdewH2cOXOmxv+Arvwk75zz236ts5qWLl3qdzs7O1uSNGjQIEnSiBEj1KBBA2VlZQXcr3Ou0lNdr6juKalXgvPWW2/5bd+0aZPOnTunO++885rztaFVq1ZKTk7WsmXLAgIvScXFxfbfP/rRj7Rjxw598sknfr+/atWqKh9n5MiROnnypJYsWRLwe1ee8yv/P8zVP3zU1hqre0pq+/btddttt2n16tV+fz+OHDmiv//973Xyut1KOFKopx555BFNnjxZI0eO1P3336/c3Fy99957ioqK8ttv9uzZ2rRpk4YMGaKMjAwlJibq3Llz2rNnj9atW6fCwsKAmasVFxdr4cKFAdvj4uI0evRo9e3bVy+++KLKysoUExOjzZs32/n4FSkoKNCwYcM0cOBAbd++XW+++aZ+8pOfqHv37pIuB2/hwoXKzMxUYWGhHnroId1+++0qKCjQxo0bNWnSJM2aNavS+6/uKalDhw5Vly5d9Mwzz+jzzz9Xr169dODAAS1ZskStW7fWxIkTbd/CwkLFxcVp3LhxWrFixTWfr5paunSpevfurW7duunRRx9Vu3btdPz4cW3fvl1HjhxRbm6uJGnOnDl64403NHDgQM2YMcNO94yNjdXu3buv+Rhjx47VypUr9dOf/lSffPKJ+vTpo3Pnzun999/X1KlT9eCDD6px48bq3LmzVq9erQ4dOigyMlJdu3ZV165da2WN1T0ltWXLlpowYYJ+//vfKyUlRSNGjFBJSYleeeUVXbhwQZmZmTV7AXBtdXPS062nslNSKzsd9NKlS+7nP/+5i4qKcuHh4W7AgAHuwIEDAaekOudcSUmJy8zMdO3bt3eNGjVyUVFR7t5773WLFy92X3311TXXdeW02Ip+paSkOOecO3LkiBs+fLhr3ry5u+OOO1xqaqorKioKOJ3xyimp+/btc6NGjXK33367i4iIcNOnT3cXLlwIeOz169e73r17uyZNmrgmTZq47373u27atGnus88+s31qekrq6dOn3cyZM12HDh1caGioi4qKcunp6e7QoUN+++3Zs8dJcnPnzq3yPmNjY93gwYOvuc+VU1IXLVpU4e8fPHjQjR071kVHR7uGDRu6mJgYN2TIELdu3Tq//Xbv3u2SkpJcWFiYi4mJcQsWLHB/+MMfqjwl1bnLpxI/9dRTLi4uzjVs2NBFR0e7UaNGuYMHD9o+27Ztc4mJia5Ro0YBr+f1XqOXU1LLyspcdna269Gjh2vatKlr2rSpu++++9yHH35Y5SxqxufcVcfvwC3olVde0Zw5c3Tw4EHP/wMhcDPhOwVA0tatW/XEE08QBNzyOFIAABiOFAAAhigAAAxRAAAYogAAMNX+n9equrIiAKB+q855RRwpAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgQup6AUB90bVrV88zzz33nOeZzp07e56RpHbt2nmemTp1queZd9991/NMYWGh5xnUTxwpAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgfM45V60dfb7aXgtw3SQmJnqe2bx5s+eZ5s2be575Jn355ZeeZ4K5IF5GRobnmQsXLnieQc1U5+2eIwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAExIXS8AqEqTJk08zyxZssTzTHh4uOeZBQsWeJ7ZsGGD55lgBXNBvLCwsFpYSaDU1NSg5vbt2+d5Ji8vL6jHuhVxpAAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgOGCeKj3unXr5nnm7rvv9jyTmZnpeebFF1/0PIPLJk6cGNTcqlWrPM9wQbzq40gBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhqukot7r3bu35xmfz+d5hiueBm/69OmeZx544IGgHmvx4sVBzaF6OFIAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMBwQTzUeyNHjvQ88+mnn9bCSm4Nf/7znz3PDBw40PPMj3/8Y88zkvT+++8HNYfq4UgBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAADDBfFQ74WFhXmeKSkpqYWV3HheeuklzzPBXNxu1apVnmdyc3M9z6D2caQAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIDhgnjADWLSpEmeZ2bOnOl5ZtmyZZ5n5syZ43mGixbWTxwpAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwPicc65aO/p8tb0W3OS6d+8e1NyuXbuu80oq1qZNG88zRUVFnmcGDRrkeUaS3nnnHc8zK1eu9DyTkZHheQY3huq83XOkAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCACanrBeDWEczF4yQpLy/P80znzp09z6SkpHieiYiI8DyzaNEizzOS9Omnn3qemTJlSlCPhVsXRwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABifc85Va0efr7bXAlRo4sSJnmcmT57seebixYueZxISEjzPbN++3fOMJE2dOtXzzOHDh4N6LNycqvN2z5ECAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAAAmpK4XAFQlIiLC80zPnj09zwRz0cd//etfnmeGDh3qeQb4pnCkAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCA4YJ4qPdat27teSaYi9vt3bvX88yYMWM8zwD1GUcKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMFwlFd+Yhx9+OKi56dOne55xznmeKSkp8TyTn5/veQaozzhSAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDA+Fw1rxzm8/lqey24gYSGhnqeyc3NDeqxzpw543mmuLjY88zgwYM9z6SkpHie2bp1q+cZ4Hqozts9RwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAAJiQul4AbkxZWVmeZxISEoJ6rMWLF3ue2bt3r+eZYC6I17NnT88zXBAP9RlHCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGC6Ih6CkpqZ6nvH5fEE9VlFRkeeZLl26eJ4Jdn3AzYQjBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADBfEgyIiIjzPxMXFeZ5xznmekaTnnnvO80xYWJjnmWDXB9xMOFIAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCA4SqpqPeCueJpMHbt2uV5Zs2aNbWwEqDucKQAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIDxOedctXb0+Wp7LbiBPPbYY55n+vXrF9RjBXNBvGAubjd//nzPM8CNpDpv9xwpAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBguCAeANwiuCAeAMATogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAhFR3R+dcba4DAFAPcKQAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADD/B4Z6zZufYoG2AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}